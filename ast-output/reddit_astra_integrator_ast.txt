AST for temp_dir\ARTGuru-AI-main\python_webscraper_integarted_with_astradb\reddit_astra_integrator.py
==================================================
module [import praw
import json
import os
from datetime import datetime
import numpy as np
from astrapy import DataAPIClient


class RedditAstraIntegrator:
    def __init__(self, reddit_client_id, reddit_client_secret, reddit_user_agent,
                 astra_token, astra_endpoint, collection_name="you",
                 output_dir="reddit_data"):
        """Initialize Reddit scraper and AstraDB connection."""
        # Initialize Reddit client
        self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )

        # Initialize AstraDB
        self.astra_client = DataAPIClient(astra_token)
        self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)
        self.collection = self.db[collection_name]
        self.vector_dim = 1536

        # Setup output directory
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def create_vector(self, text: str) -> list:
        """Create normalized vector for text embedding."""
        vector = np.random.normal(0, 1, self.vector_dim)
        normalized_vector = vector / np.linalg.norm(vector)
        return normalized_vector.tolist()

    def prepare_document(self, post_info: dict) -> dict:
        """Prepare Reddit post data for AstraDB storage."""
        try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None

    def search_and_store_posts(self, topic: str, limit: int = 5) -> dict:
        """Search for Reddit posts and store them in AstraDB."""
        results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }

        try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results

    def _generate_subreddits_for_topic(self, topic: str) -> list:
        """Generate relevant subreddits for a topic."""
        topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }

        topic_lower = topic.lower()
        return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])

    def _parse_comments_recursively(self, comments, depth=0, max_depth=3):
        """Parse comments recursively with depth limit."""
        if depth >= max_depth:
            return []

        parsed_comments = []
        for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")

        return parsed_comments

    def _save_local_copy(self, posts: list, topic: str) -> str:
        """Save local copy of posts."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{topic}_posts_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (0:0-204:23)
  import_statement [import praw] (0:0-0:11)
    import [import] (0:0-0:6)
    dotted_name [praw] (0:7-0:11)
      identifier [praw] (0:7-0:11)
  import_statement [import json] (1:0-1:11)
    import [import] (1:0-1:6)
    dotted_name [json] (1:7-1:11)
      identifier [json] (1:7-1:11)
  import_statement [import os] (2:0-2:9)
    import [import] (2:0-2:6)
    dotted_name [os] (2:7-2:9)
      identifier [os] (2:7-2:9)
  import_from_statement [from datetime import datetime] (3:0-3:29)
    from [from] (3:0-3:4)
    dotted_name [datetime] (3:5-3:13)
      identifier [datetime] (3:5-3:13)
    import [import] (3:14-3:20)
    dotted_name [datetime] (3:21-3:29)
      identifier [datetime] (3:21-3:29)
  import_statement [import numpy as np] (4:0-4:18)
    import [import] (4:0-4:6)
    aliased_import [numpy as np] (4:7-4:18)
      dotted_name [numpy] (4:7-4:12)
        identifier [numpy] (4:7-4:12)
      as [as] (4:13-4:15)
      identifier [np] (4:16-4:18)
  import_from_statement [from astrapy import DataAPIClient] (5:0-5:33)
    from [from] (5:0-5:4)
    dotted_name [astrapy] (5:5-5:12)
      identifier [astrapy] (5:5-5:12)
    import [import] (5:13-5:19)
    dotted_name [DataAPIClient] (5:20-5:33)
      identifier [DataAPIClient] (5:20-5:33)
  class_definition [class RedditAstraIntegrator:
    def __init__(self, reddit_client_id, reddit_client_secret, reddit_user_agent,
                 astra_token, astra_endpoint, collection_name="you",
                 output_dir="reddit_data"):
        """Initialize Reddit scraper and AstraDB connection."""
        # Initialize Reddit client
        self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )

        # Initialize AstraDB
        self.astra_client = DataAPIClient(astra_token)
        self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)
        self.collection = self.db[collection_name]
        self.vector_dim = 1536

        # Setup output directory
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def create_vector(self, text: str) -> list:
        """Create normalized vector for text embedding."""
        vector = np.random.normal(0, 1, self.vector_dim)
        normalized_vector = vector / np.linalg.norm(vector)
        return normalized_vector.tolist()

    def prepare_document(self, post_info: dict) -> dict:
        """Prepare Reddit post data for AstraDB storage."""
        try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None

    def search_and_store_posts(self, topic: str, limit: int = 5) -> dict:
        """Search for Reddit posts and store them in AstraDB."""
        results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }

        try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results

    def _generate_subreddits_for_topic(self, topic: str) -> list:
        """Generate relevant subreddits for a topic."""
        topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }

        topic_lower = topic.lower()
        return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])

    def _parse_comments_recursively(self, comments, depth=0, max_depth=3):
        """Parse comments recursively with depth limit."""
        if depth >= max_depth:
            return []

        parsed_comments = []
        for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")

        return parsed_comments

    def _save_local_copy(self, posts: list, topic: str) -> str:
        """Save local copy of posts."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{topic}_posts_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (8:0-204:23)
    class [class] (8:0-8:5)
    identifier [RedditAstraIntegrator] (8:6-8:27)
    : [:] (8:27-8:28)
    block [def __init__(self, reddit_client_id, reddit_client_secret, reddit_user_agent,
                 astra_token, astra_endpoint, collection_name="you",
                 output_dir="reddit_data"):
        """Initialize Reddit scraper and AstraDB connection."""
        # Initialize Reddit client
        self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )

        # Initialize AstraDB
        self.astra_client = DataAPIClient(astra_token)
        self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)
        self.collection = self.db[collection_name]
        self.vector_dim = 1536

        # Setup output directory
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def create_vector(self, text: str) -> list:
        """Create normalized vector for text embedding."""
        vector = np.random.normal(0, 1, self.vector_dim)
        normalized_vector = vector / np.linalg.norm(vector)
        return normalized_vector.tolist()

    def prepare_document(self, post_info: dict) -> dict:
        """Prepare Reddit post data for AstraDB storage."""
        try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None

    def search_and_store_posts(self, topic: str, limit: int = 5) -> dict:
        """Search for Reddit posts and store them in AstraDB."""
        results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }

        try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results

    def _generate_subreddits_for_topic(self, topic: str) -> list:
        """Generate relevant subreddits for a topic."""
        topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }

        topic_lower = topic.lower()
        return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])

    def _parse_comments_recursively(self, comments, depth=0, max_depth=3):
        """Parse comments recursively with depth limit."""
        if depth >= max_depth:
            return []

        parsed_comments = []
        for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")

        return parsed_comments

    def _save_local_copy(self, posts: list, topic: str) -> str:
        """Save local copy of posts."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{topic}_posts_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (9:4-204:23)
      function_definition [def __init__(self, reddit_client_id, reddit_client_secret, reddit_user_agent,
                 astra_token, astra_endpoint, collection_name="you",
                 output_dir="reddit_data"):
        """Initialize Reddit scraper and AstraDB connection."""
        # Initialize Reddit client
        self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )

        # Initialize AstraDB
        self.astra_client = DataAPIClient(astra_token)
        self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)
        self.collection = self.db[collection_name]
        self.vector_dim = 1536

        # Setup output directory
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)] (9:4-29:35)
        def [def] (9:4-9:7)
        identifier [__init__] (9:8-9:16)
        parameters [(self, reddit_client_id, reddit_client_secret, reddit_user_agent,
                 astra_token, astra_endpoint, collection_name="you",
                 output_dir="reddit_data")] (9:16-11:42)
          ( [(] (9:16-9:17)
          identifier [self] (9:17-9:21)
          , [,] (9:21-9:22)
          identifier [reddit_client_id] (9:23-9:39)
          , [,] (9:39-9:40)
          identifier [reddit_client_secret] (9:41-9:61)
          , [,] (9:61-9:62)
          identifier [reddit_user_agent] (9:63-9:80)
          , [,] (9:80-9:81)
          identifier [astra_token] (10:17-10:28)
          , [,] (10:28-10:29)
          identifier [astra_endpoint] (10:30-10:44)
          , [,] (10:44-10:45)
          default_parameter [collection_name="you"] (10:46-10:67)
            identifier [collection_name] (10:46-10:61)
            = [=] (10:61-10:62)
            string ["you"] (10:62-10:67)
              string_start ["] (10:62-10:63)
              string_content [you] (10:63-10:66)
              string_end ["] (10:66-10:67)
          , [,] (10:67-10:68)
          default_parameter [output_dir="reddit_data"] (11:17-11:41)
            identifier [output_dir] (11:17-11:27)
            = [=] (11:27-11:28)
            string ["reddit_data"] (11:28-11:41)
              string_start ["] (11:28-11:29)
              string_content [reddit_data] (11:29-11:40)
              string_end ["] (11:40-11:41)
          ) [)] (11:41-11:42)
        : [:] (11:42-11:43)
        block ["""Initialize Reddit scraper and AstraDB connection."""
        # Initialize Reddit client
        self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )

        # Initialize AstraDB
        self.astra_client = DataAPIClient(astra_token)
        self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)
        self.collection = self.db[collection_name]
        self.vector_dim = 1536

        # Setup output directory
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)] (12:8-29:35)
          expression_statement ["""Initialize Reddit scraper and AstraDB connection."""] (12:8-12:63)
            string ["""Initialize Reddit scraper and AstraDB connection."""] (12:8-12:63)
              string_start ["""] (12:8-12:11)
              string_content [Initialize Reddit scraper and AstraDB connection.] (12:11-12:60)
              string_end ["""] (12:60-12:63)
          comment [# Initialize Reddit client] (13:8-13:34)
          expression_statement [self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )] (14:8-18:9)
            assignment [self.reddit = praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )] (14:8-18:9)
              attribute [self.reddit] (14:8-14:19)
                identifier [self] (14:8-14:12)
                . [.] (14:12-14:13)
                identifier [reddit] (14:13-14:19)
              = [=] (14:20-14:21)
              call [praw.Reddit(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )] (14:22-18:9)
                attribute [praw.Reddit] (14:22-14:33)
                  identifier [praw] (14:22-14:26)
                  . [.] (14:26-14:27)
                  identifier [Reddit] (14:27-14:33)
                argument_list [(
            client_id=reddit_client_id,
            client_secret=reddit_client_secret,
            user_agent=reddit_user_agent
        )] (14:33-18:9)
                  ( [(] (14:33-14:34)
                  keyword_argument [client_id=reddit_client_id] (15:12-15:38)
                    identifier [client_id] (15:12-15:21)
                    = [=] (15:21-15:22)
                    identifier [reddit_client_id] (15:22-15:38)
                  , [,] (15:38-15:39)
                  keyword_argument [client_secret=reddit_client_secret] (16:12-16:46)
                    identifier [client_secret] (16:12-16:25)
                    = [=] (16:25-16:26)
                    identifier [reddit_client_secret] (16:26-16:46)
                  , [,] (16:46-16:47)
                  keyword_argument [user_agent=reddit_user_agent] (17:12-17:40)
                    identifier [user_agent] (17:12-17:22)
                    = [=] (17:22-17:23)
                    identifier [reddit_user_agent] (17:23-17:40)
                  ) [)] (18:8-18:9)
          comment [# Initialize AstraDB] (20:8-20:28)
          expression_statement [self.astra_client = DataAPIClient(astra_token)] (21:8-21:54)
            assignment [self.astra_client = DataAPIClient(astra_token)] (21:8-21:54)
              attribute [self.astra_client] (21:8-21:25)
                identifier [self] (21:8-21:12)
                . [.] (21:12-21:13)
                identifier [astra_client] (21:13-21:25)
              = [=] (21:26-21:27)
              call [DataAPIClient(astra_token)] (21:28-21:54)
                identifier [DataAPIClient] (21:28-21:41)
                argument_list [(astra_token)] (21:41-21:54)
                  ( [(] (21:41-21:42)
                  identifier [astra_token] (21:42-21:53)
                  ) [)] (21:53-21:54)
          expression_statement [self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)] (22:8-22:80)
            assignment [self.db = self.astra_client.get_database_by_api_endpoint(astra_endpoint)] (22:8-22:80)
              attribute [self.db] (22:8-22:15)
                identifier [self] (22:8-22:12)
                . [.] (22:12-22:13)
                identifier [db] (22:13-22:15)
              = [=] (22:16-22:17)
              call [self.astra_client.get_database_by_api_endpoint(astra_endpoint)] (22:18-22:80)
                attribute [self.astra_client.get_database_by_api_endpoint] (22:18-22:64)
                  attribute [self.astra_client] (22:18-22:35)
                    identifier [self] (22:18-22:22)
                    . [.] (22:22-22:23)
                    identifier [astra_client] (22:23-22:35)
                  . [.] (22:35-22:36)
                  identifier [get_database_by_api_endpoint] (22:36-22:64)
                argument_list [(astra_endpoint)] (22:64-22:80)
                  ( [(] (22:64-22:65)
                  identifier [astra_endpoint] (22:65-22:79)
                  ) [)] (22:79-22:80)
          expression_statement [self.collection = self.db[collection_name]] (23:8-23:50)
            assignment [self.collection = self.db[collection_name]] (23:8-23:50)
              attribute [self.collection] (23:8-23:23)
                identifier [self] (23:8-23:12)
                . [.] (23:12-23:13)
                identifier [collection] (23:13-23:23)
              = [=] (23:24-23:25)
              subscript [self.db[collection_name]] (23:26-23:50)
                attribute [self.db] (23:26-23:33)
                  identifier [self] (23:26-23:30)
                  . [.] (23:30-23:31)
                  identifier [db] (23:31-23:33)
                [ [[] (23:33-23:34)
                identifier [collection_name] (23:34-23:49)
                ] []] (23:49-23:50)
          expression_statement [self.vector_dim = 1536] (24:8-24:30)
            assignment [self.vector_dim = 1536] (24:8-24:30)
              attribute [self.vector_dim] (24:8-24:23)
                identifier [self] (24:8-24:12)
                . [.] (24:12-24:13)
                identifier [vector_dim] (24:13-24:23)
              = [=] (24:24-24:25)
              integer [1536] (24:26-24:30)
          comment [# Setup output directory] (26:8-26:32)
          expression_statement [self.output_dir = output_dir] (27:8-27:36)
            assignment [self.output_dir = output_dir] (27:8-27:36)
              attribute [self.output_dir] (27:8-27:23)
                identifier [self] (27:8-27:12)
                . [.] (27:12-27:13)
                identifier [output_dir] (27:13-27:23)
              = [=] (27:24-27:25)
              identifier [output_dir] (27:26-27:36)
          if_statement [if not os.path.exists(output_dir):
            os.makedirs(output_dir)] (28:8-29:35)
            if [if] (28:8-28:10)
            not_operator [not os.path.exists(output_dir)] (28:11-28:41)
              not [not] (28:11-28:14)
              call [os.path.exists(output_dir)] (28:15-28:41)
                attribute [os.path.exists] (28:15-28:29)
                  attribute [os.path] (28:15-28:22)
                    identifier [os] (28:15-28:17)
                    . [.] (28:17-28:18)
                    identifier [path] (28:18-28:22)
                  . [.] (28:22-28:23)
                  identifier [exists] (28:23-28:29)
                argument_list [(output_dir)] (28:29-28:41)
                  ( [(] (28:29-28:30)
                  identifier [output_dir] (28:30-28:40)
                  ) [)] (28:40-28:41)
            : [:] (28:41-28:42)
            block [os.makedirs(output_dir)] (29:12-29:35)
              expression_statement [os.makedirs(output_dir)] (29:12-29:35)
                call [os.makedirs(output_dir)] (29:12-29:35)
                  attribute [os.makedirs] (29:12-29:23)
                    identifier [os] (29:12-29:14)
                    . [.] (29:14-29:15)
                    identifier [makedirs] (29:15-29:23)
                  argument_list [(output_dir)] (29:23-29:35)
                    ( [(] (29:23-29:24)
                    identifier [output_dir] (29:24-29:34)
                    ) [)] (29:34-29:35)
      function_definition [def create_vector(self, text: str) -> list:
        """Create normalized vector for text embedding."""
        vector = np.random.normal(0, 1, self.vector_dim)
        normalized_vector = vector / np.linalg.norm(vector)
        return normalized_vector.tolist()] (31:4-35:41)
        def [def] (31:4-31:7)
        identifier [create_vector] (31:8-31:21)
        parameters [(self, text: str)] (31:21-31:38)
          ( [(] (31:21-31:22)
          identifier [self] (31:22-31:26)
          , [,] (31:26-31:27)
          typed_parameter [text: str] (31:28-31:37)
            identifier [text] (31:28-31:32)
            : [:] (31:32-31:33)
            type [str] (31:34-31:37)
              identifier [str] (31:34-31:37)
          ) [)] (31:37-31:38)
        -> [->] (31:39-31:41)
        type [list] (31:42-31:46)
          identifier [list] (31:42-31:46)
        : [:] (31:46-31:47)
        block ["""Create normalized vector for text embedding."""
        vector = np.random.normal(0, 1, self.vector_dim)
        normalized_vector = vector / np.linalg.norm(vector)
        return normalized_vector.tolist()] (32:8-35:41)
          expression_statement ["""Create normalized vector for text embedding."""] (32:8-32:58)
            string ["""Create normalized vector for text embedding."""] (32:8-32:58)
              string_start ["""] (32:8-32:11)
              string_content [Create normalized vector for text embedding.] (32:11-32:55)
              string_end ["""] (32:55-32:58)
          expression_statement [vector = np.random.normal(0, 1, self.vector_dim)] (33:8-33:56)
            assignment [vector = np.random.normal(0, 1, self.vector_dim)] (33:8-33:56)
              identifier [vector] (33:8-33:14)
              = [=] (33:15-33:16)
              call [np.random.normal(0, 1, self.vector_dim)] (33:17-33:56)
                attribute [np.random.normal] (33:17-33:33)
                  attribute [np.random] (33:17-33:26)
                    identifier [np] (33:17-33:19)
                    . [.] (33:19-33:20)
                    identifier [random] (33:20-33:26)
                  . [.] (33:26-33:27)
                  identifier [normal] (33:27-33:33)
                argument_list [(0, 1, self.vector_dim)] (33:33-33:56)
                  ( [(] (33:33-33:34)
                  integer [0] (33:34-33:35)
                  , [,] (33:35-33:36)
                  integer [1] (33:37-33:38)
                  , [,] (33:38-33:39)
                  attribute [self.vector_dim] (33:40-33:55)
                    identifier [self] (33:40-33:44)
                    . [.] (33:44-33:45)
                    identifier [vector_dim] (33:45-33:55)
                  ) [)] (33:55-33:56)
          expression_statement [normalized_vector = vector / np.linalg.norm(vector)] (34:8-34:59)
            assignment [normalized_vector = vector / np.linalg.norm(vector)] (34:8-34:59)
              identifier [normalized_vector] (34:8-34:25)
              = [=] (34:26-34:27)
              binary_operator [vector / np.linalg.norm(vector)] (34:28-34:59)
                identifier [vector] (34:28-34:34)
                / [/] (34:35-34:36)
                call [np.linalg.norm(vector)] (34:37-34:59)
                  attribute [np.linalg.norm] (34:37-34:51)
                    attribute [np.linalg] (34:37-34:46)
                      identifier [np] (34:37-34:39)
                      . [.] (34:39-34:40)
                      identifier [linalg] (34:40-34:46)
                    . [.] (34:46-34:47)
                    identifier [norm] (34:47-34:51)
                  argument_list [(vector)] (34:51-34:59)
                    ( [(] (34:51-34:52)
                    identifier [vector] (34:52-34:58)
                    ) [)] (34:58-34:59)
          return_statement [return normalized_vector.tolist()] (35:8-35:41)
            return [return] (35:8-35:14)
            call [normalized_vector.tolist()] (35:15-35:41)
              attribute [normalized_vector.tolist] (35:15-35:39)
                identifier [normalized_vector] (35:15-35:32)
                . [.] (35:32-35:33)
                identifier [tolist] (35:33-35:39)
              argument_list [()] (35:39-35:41)
                ( [(] (35:39-35:40)
                ) [)] (35:40-35:41)
      function_definition [def prepare_document(self, post_info: dict) -> dict:
        """Prepare Reddit post data for AstraDB storage."""
        try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None] (37:4-81:23)
        def [def] (37:4-37:7)
        identifier [prepare_document] (37:8-37:24)
        parameters [(self, post_info: dict)] (37:24-37:47)
          ( [(] (37:24-37:25)
          identifier [self] (37:25-37:29)
          , [,] (37:29-37:30)
          typed_parameter [post_info: dict] (37:31-37:46)
            identifier [post_info] (37:31-37:40)
            : [:] (37:40-37:41)
            type [dict] (37:42-37:46)
              identifier [dict] (37:42-37:46)
          ) [)] (37:46-37:47)
        -> [->] (37:48-37:50)
        type [dict] (37:51-37:55)
          identifier [dict] (37:51-37:55)
        : [:] (37:55-37:56)
        block ["""Prepare Reddit post data for AstraDB storage."""
        try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None] (38:8-81:23)
          expression_statement ["""Prepare Reddit post data for AstraDB storage."""] (38:8-38:59)
            string ["""Prepare Reddit post data for AstraDB storage."""] (38:8-38:59)
              string_start ["""] (38:8-38:11)
              string_content [Prepare Reddit post data for AstraDB storage.] (38:11-38:56)
              string_end ["""] (38:56-38:59)
          try_statement [try:
            # Combine text content for vector creation
            text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }
        except Exception as e:
            print(f"Error preparing document: {e}")
            return None] (39:8-81:23)
            try [try] (39:8-39:11)
            : [:] (39:11-39:12)
            comment [# Combine text content for vector creation] (40:12-40:54)
            block [text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "
            text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])

            # Truncate content if needed
            content_bytes = text_content.encode('utf-8')
            if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')

            # Create metadata with essential fields
            metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }

            # Add truncated comments
            if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]

            return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }] (41:12-78:13)
              expression_statement [text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "] (41:12-41:91)
                assignment [text_content = f"{post_info.get('title', '')} {post_info.get('selftext', '')} "] (41:12-41:91)
                  identifier [text_content] (41:12-41:24)
                  = [=] (41:25-41:26)
                  string [f"{post_info.get('title', '')} {post_info.get('selftext', '')} "] (41:27-41:91)
                    string_start [f"] (41:27-41:29)
                    interpolation [{post_info.get('title', '')}] (41:29-41:57)
                      { [{] (41:29-41:30)
                      call [post_info.get('title', '')] (41:30-41:56)
                        attribute [post_info.get] (41:30-41:43)
                          identifier [post_info] (41:30-41:39)
                          . [.] (41:39-41:40)
                          identifier [get] (41:40-41:43)
                        argument_list [('title', '')] (41:43-41:56)
                          ( [(] (41:43-41:44)
                          string ['title'] (41:44-41:51)
                            string_start ['] (41:44-41:45)
                            string_content [title] (41:45-41:50)
                            string_end ['] (41:50-41:51)
                          , [,] (41:51-41:52)
                          string [''] (41:53-41:55)
                            string_start ['] (41:53-41:54)
                            string_end ['] (41:54-41:55)
                          ) [)] (41:55-41:56)
                      } [}] (41:56-41:57)
                    string_content [ ] (41:57-41:58)
                    interpolation [{post_info.get('selftext', '')}] (41:58-41:89)
                      { [{] (41:58-41:59)
                      call [post_info.get('selftext', '')] (41:59-41:88)
                        attribute [post_info.get] (41:59-41:72)
                          identifier [post_info] (41:59-41:68)
                          . [.] (41:68-41:69)
                          identifier [get] (41:69-41:72)
                        argument_list [('selftext', '')] (41:72-41:88)
                          ( [(] (41:72-41:73)
                          string ['selftext'] (41:73-41:83)
                            string_start ['] (41:73-41:74)
                            string_content [selftext] (41:74-41:82)
                            string_end ['] (41:82-41:83)
                          , [,] (41:83-41:84)
                          string [''] (41:85-41:87)
                            string_start ['] (41:85-41:86)
                            string_end ['] (41:86-41:87)
                          ) [)] (41:87-41:88)
                      } [}] (41:88-41:89)
                    string_content [ ] (41:89-41:90)
                    string_end ["] (41:90-41:91)
              expression_statement [text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])] (42:12-42:100)
                augmented_assignment [text_content += ' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])] (42:12-42:100)
                  identifier [text_content] (42:12-42:24)
                  += [+=] (42:25-42:27)
                  call [' '.join([c.get('body', '') for c in post_info.get('comments', [])][:5])] (42:28-42:100)
                    attribute [' '.join] (42:28-42:36)
                      string [' '] (42:28-42:31)
                        string_start ['] (42:28-42:29)
                        string_content [ ] (42:29-42:30)
                        string_end ['] (42:30-42:31)
                      . [.] (42:31-42:32)
                      identifier [join] (42:32-42:36)
                    argument_list [([c.get('body', '') for c in post_info.get('comments', [])][:5])] (42:36-42:100)
                      ( [(] (42:36-42:37)
                      subscript [[c.get('body', '') for c in post_info.get('comments', [])][:5]] (42:37-42:99)
                        list_comprehension [[c.get('body', '') for c in post_info.get('comments', [])]] (42:37-42:95)
                          [ [[] (42:37-42:38)
                          call [c.get('body', '')] (42:38-42:55)
                            attribute [c.get] (42:38-42:43)
                              identifier [c] (42:38-42:39)
                              . [.] (42:39-42:40)
                              identifier [get] (42:40-42:43)
                            argument_list [('body', '')] (42:43-42:55)
                              ( [(] (42:43-42:44)
                              string ['body'] (42:44-42:50)
                                string_start ['] (42:44-42:45)
                                string_content [body] (42:45-42:49)
                                string_end ['] (42:49-42:50)
                              , [,] (42:50-42:51)
                              string [''] (42:52-42:54)
                                string_start ['] (42:52-42:53)
                                string_end ['] (42:53-42:54)
                              ) [)] (42:54-42:55)
                          for_in_clause [for c in post_info.get('comments', [])] (42:56-42:94)
                            for [for] (42:56-42:59)
                            identifier [c] (42:60-42:61)
                            in [in] (42:62-42:64)
                            call [post_info.get('comments', [])] (42:65-42:94)
                              attribute [post_info.get] (42:65-42:78)
                                identifier [post_info] (42:65-42:74)
                                . [.] (42:74-42:75)
                                identifier [get] (42:75-42:78)
                              argument_list [('comments', [])] (42:78-42:94)
                                ( [(] (42:78-42:79)
                                string ['comments'] (42:79-42:89)
                                  string_start ['] (42:79-42:80)
                                  string_content [comments] (42:80-42:88)
                                  string_end ['] (42:88-42:89)
                                , [,] (42:89-42:90)
                                list [[]] (42:91-42:93)
                                  [ [[] (42:91-42:92)
                                  ] []] (42:92-42:93)
                                ) [)] (42:93-42:94)
                          ] []] (42:94-42:95)
                        [ [[] (42:95-42:96)
                        slice [:5] (42:96-42:98)
                          : [:] (42:96-42:97)
                          integer [5] (42:97-42:98)
                        ] []] (42:98-42:99)
                      ) [)] (42:99-42:100)
              comment [# Truncate content if needed] (44:12-44:40)
              expression_statement [content_bytes = text_content.encode('utf-8')] (45:12-45:56)
                assignment [content_bytes = text_content.encode('utf-8')] (45:12-45:56)
                  identifier [content_bytes] (45:12-45:25)
                  = [=] (45:26-45:27)
                  call [text_content.encode('utf-8')] (45:28-45:56)
                    attribute [text_content.encode] (45:28-45:47)
                      identifier [text_content] (45:28-45:40)
                      . [.] (45:40-45:41)
                      identifier [encode] (45:41-45:47)
                    argument_list [('utf-8')] (45:47-45:56)
                      ( [(] (45:47-45:48)
                      string ['utf-8'] (45:48-45:55)
                        string_start ['] (45:48-45:49)
                        string_content [utf-8] (45:49-45:54)
                        string_end ['] (45:54-45:55)
                      ) [)] (45:55-45:56)
              if_statement [if len(content_bytes) > 8000:
                content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')] (46:12-48:77)
                if [if] (46:12-46:14)
                comparison_operator [len(content_bytes) > 8000] (46:15-46:40)
                  call [len(content_bytes)] (46:15-46:33)
                    identifier [len] (46:15-46:18)
                    argument_list [(content_bytes)] (46:18-46:33)
                      ( [(] (46:18-46:19)
                      identifier [content_bytes] (46:19-46:32)
                      ) [)] (46:32-46:33)
                  > [>] (46:34-46:35)
                  integer [8000] (46:36-46:40)
                : [:] (46:40-46:41)
                block [content_bytes = content_bytes[:8000]
                text_content = content_bytes.decode('utf-8', errors='ignore')] (47:16-48:77)
                  expression_statement [content_bytes = content_bytes[:8000]] (47:16-47:52)
                    assignment [content_bytes = content_bytes[:8000]] (47:16-47:52)
                      identifier [content_bytes] (47:16-47:29)
                      = [=] (47:30-47:31)
                      subscript [content_bytes[:8000]] (47:32-47:52)
                        identifier [content_bytes] (47:32-47:45)
                        [ [[] (47:45-47:46)
                        slice [:8000] (47:46-47:51)
                          : [:] (47:46-47:47)
                          integer [8000] (47:47-47:51)
                        ] []] (47:51-47:52)
                  expression_statement [text_content = content_bytes.decode('utf-8', errors='ignore')] (48:16-48:77)
                    assignment [text_content = content_bytes.decode('utf-8', errors='ignore')] (48:16-48:77)
                      identifier [text_content] (48:16-48:28)
                      = [=] (48:29-48:30)
                      call [content_bytes.decode('utf-8', errors='ignore')] (48:31-48:77)
                        attribute [content_bytes.decode] (48:31-48:51)
                          identifier [content_bytes] (48:31-48:44)
                          . [.] (48:44-48:45)
                          identifier [decode] (48:45-48:51)
                        argument_list [('utf-8', errors='ignore')] (48:51-48:77)
                          ( [(] (48:51-48:52)
                          string ['utf-8'] (48:52-48:59)
                            string_start ['] (48:52-48:53)
                            string_content [utf-8] (48:53-48:58)
                            string_end ['] (48:58-48:59)
                          , [,] (48:59-48:60)
                          keyword_argument [errors='ignore'] (48:61-48:76)
                            identifier [errors] (48:61-48:67)
                            = [=] (48:67-48:68)
                            string ['ignore'] (48:68-48:76)
                              string_start ['] (48:68-48:69)
                              string_content [ignore] (48:69-48:75)
                              string_end ['] (48:75-48:76)
                          ) [)] (48:76-48:77)
              comment [# Create metadata with essential fields] (50:12-50:51)
              expression_statement [metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }] (51:12-61:13)
                assignment [metadata = {
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }] (51:12-61:13)
                  identifier [metadata] (51:12-51:20)
                  = [=] (51:21-51:22)
                  dictionary [{
                'title': post_info.get('title', '')[:500],
                'url': post_info.get('url', '')[:500],
                'subreddit': post_info.get('subreddit', ''),
                'author': post_info.get('author', ''),
                'score': post_info.get('score', 0),
                'num_comments': post_info.get('num_comments', 0),
                'created_utc': post_info.get('created_utc'),
                'topic': post_info.get('topic', ''),
                'scraped_at': datetime.now().isoformat()
            }] (51:23-61:13)
                    { [{] (51:23-51:24)
                    pair ['title': post_info.get('title', '')[:500]] (52:16-52:57)
                      string ['title'] (52:16-52:23)
                        string_start ['] (52:16-52:17)
                        string_content [title] (52:17-52:22)
                        string_end ['] (52:22-52:23)
                      : [:] (52:23-52:24)
                      subscript [post_info.get('title', '')[:500]] (52:25-52:57)
                        call [post_info.get('title', '')] (52:25-52:51)
                          attribute [post_info.get] (52:25-52:38)
                            identifier [post_info] (52:25-52:34)
                            . [.] (52:34-52:35)
                            identifier [get] (52:35-52:38)
                          argument_list [('title', '')] (52:38-52:51)
                            ( [(] (52:38-52:39)
                            string ['title'] (52:39-52:46)
                              string_start ['] (52:39-52:40)
                              string_content [title] (52:40-52:45)
                              string_end ['] (52:45-52:46)
                            , [,] (52:46-52:47)
                            string [''] (52:48-52:50)
                              string_start ['] (52:48-52:49)
                              string_end ['] (52:49-52:50)
                            ) [)] (52:50-52:51)
                        [ [[] (52:51-52:52)
                        slice [:500] (52:52-52:56)
                          : [:] (52:52-52:53)
                          integer [500] (52:53-52:56)
                        ] []] (52:56-52:57)
                    , [,] (52:57-52:58)
                    pair ['url': post_info.get('url', '')[:500]] (53:16-53:53)
                      string ['url'] (53:16-53:21)
                        string_start ['] (53:16-53:17)
                        string_content [url] (53:17-53:20)
                        string_end ['] (53:20-53:21)
                      : [:] (53:21-53:22)
                      subscript [post_info.get('url', '')[:500]] (53:23-53:53)
                        call [post_info.get('url', '')] (53:23-53:47)
                          attribute [post_info.get] (53:23-53:36)
                            identifier [post_info] (53:23-53:32)
                            . [.] (53:32-53:33)
                            identifier [get] (53:33-53:36)
                          argument_list [('url', '')] (53:36-53:47)
                            ( [(] (53:36-53:37)
                            string ['url'] (53:37-53:42)
                              string_start ['] (53:37-53:38)
                              string_content [url] (53:38-53:41)
                              string_end ['] (53:41-53:42)
                            , [,] (53:42-53:43)
                            string [''] (53:44-53:46)
                              string_start ['] (53:44-53:45)
                              string_end ['] (53:45-53:46)
                            ) [)] (53:46-53:47)
                        [ [[] (53:47-53:48)
                        slice [:500] (53:48-53:52)
                          : [:] (53:48-53:49)
                          integer [500] (53:49-53:52)
                        ] []] (53:52-53:53)
                    , [,] (53:53-53:54)
                    pair ['subreddit': post_info.get('subreddit', '')] (54:16-54:59)
                      string ['subreddit'] (54:16-54:27)
                        string_start ['] (54:16-54:17)
                        string_content [subreddit] (54:17-54:26)
                        string_end ['] (54:26-54:27)
                      : [:] (54:27-54:28)
                      call [post_info.get('subreddit', '')] (54:29-54:59)
                        attribute [post_info.get] (54:29-54:42)
                          identifier [post_info] (54:29-54:38)
                          . [.] (54:38-54:39)
                          identifier [get] (54:39-54:42)
                        argument_list [('subreddit', '')] (54:42-54:59)
                          ( [(] (54:42-54:43)
                          string ['subreddit'] (54:43-54:54)
                            string_start ['] (54:43-54:44)
                            string_content [subreddit] (54:44-54:53)
                            string_end ['] (54:53-54:54)
                          , [,] (54:54-54:55)
                          string [''] (54:56-54:58)
                            string_start ['] (54:56-54:57)
                            string_end ['] (54:57-54:58)
                          ) [)] (54:58-54:59)
                    , [,] (54:59-54:60)
                    pair ['author': post_info.get('author', '')] (55:16-55:53)
                      string ['author'] (55:16-55:24)
                        string_start ['] (55:16-55:17)
                        string_content [author] (55:17-55:23)
                        string_end ['] (55:23-55:24)
                      : [:] (55:24-55:25)
                      call [post_info.get('author', '')] (55:26-55:53)
                        attribute [post_info.get] (55:26-55:39)
                          identifier [post_info] (55:26-55:35)
                          . [.] (55:35-55:36)
                          identifier [get] (55:36-55:39)
                        argument_list [('author', '')] (55:39-55:53)
                          ( [(] (55:39-55:40)
                          string ['author'] (55:40-55:48)
                            string_start ['] (55:40-55:41)
                            string_content [author] (55:41-55:47)
                            string_end ['] (55:47-55:48)
                          , [,] (55:48-55:49)
                          string [''] (55:50-55:52)
                            string_start ['] (55:50-55:51)
                            string_end ['] (55:51-55:52)
                          ) [)] (55:52-55:53)
                    , [,] (55:53-55:54)
                    pair ['score': post_info.get('score', 0)] (56:16-56:50)
                      string ['score'] (56:16-56:23)
                        string_start ['] (56:16-56:17)
                        string_content [score] (56:17-56:22)
                        string_end ['] (56:22-56:23)
                      : [:] (56:23-56:24)
                      call [post_info.get('score', 0)] (56:25-56:50)
                        attribute [post_info.get] (56:25-56:38)
                          identifier [post_info] (56:25-56:34)
                          . [.] (56:34-56:35)
                          identifier [get] (56:35-56:38)
                        argument_list [('score', 0)] (56:38-56:50)
                          ( [(] (56:38-56:39)
                          string ['score'] (56:39-56:46)
                            string_start ['] (56:39-56:40)
                            string_content [score] (56:40-56:45)
                            string_end ['] (56:45-56:46)
                          , [,] (56:46-56:47)
                          integer [0] (56:48-56:49)
                          ) [)] (56:49-56:50)
                    , [,] (56:50-56:51)
                    pair ['num_comments': post_info.get('num_comments', 0)] (57:16-57:64)
                      string ['num_comments'] (57:16-57:30)
                        string_start ['] (57:16-57:17)
                        string_content [num_comments] (57:17-57:29)
                        string_end ['] (57:29-57:30)
                      : [:] (57:30-57:31)
                      call [post_info.get('num_comments', 0)] (57:32-57:64)
                        attribute [post_info.get] (57:32-57:45)
                          identifier [post_info] (57:32-57:41)
                          . [.] (57:41-57:42)
                          identifier [get] (57:42-57:45)
                        argument_list [('num_comments', 0)] (57:45-57:64)
                          ( [(] (57:45-57:46)
                          string ['num_comments'] (57:46-57:60)
                            string_start ['] (57:46-57:47)
                            string_content [num_comments] (57:47-57:59)
                            string_end ['] (57:59-57:60)
                          , [,] (57:60-57:61)
                          integer [0] (57:62-57:63)
                          ) [)] (57:63-57:64)
                    , [,] (57:64-57:65)
                    pair ['created_utc': post_info.get('created_utc')] (58:16-58:59)
                      string ['created_utc'] (58:16-58:29)
                        string_start ['] (58:16-58:17)
                        string_content [created_utc] (58:17-58:28)
                        string_end ['] (58:28-58:29)
                      : [:] (58:29-58:30)
                      call [post_info.get('created_utc')] (58:31-58:59)
                        attribute [post_info.get] (58:31-58:44)
                          identifier [post_info] (58:31-58:40)
                          . [.] (58:40-58:41)
                          identifier [get] (58:41-58:44)
                        argument_list [('created_utc')] (58:44-58:59)
                          ( [(] (58:44-58:45)
                          string ['created_utc'] (58:45-58:58)
                            string_start ['] (58:45-58:46)
                            string_content [created_utc] (58:46-58:57)
                            string_end ['] (58:57-58:58)
                          ) [)] (58:58-58:59)
                    , [,] (58:59-58:60)
                    pair ['topic': post_info.get('topic', '')] (59:16-59:51)
                      string ['topic'] (59:16-59:23)
                        string_start ['] (59:16-59:17)
                        string_content [topic] (59:17-59:22)
                        string_end ['] (59:22-59:23)
                      : [:] (59:23-59:24)
                      call [post_info.get('topic', '')] (59:25-59:51)
                        attribute [post_info.get] (59:25-59:38)
                          identifier [post_info] (59:25-59:34)
                          . [.] (59:34-59:35)
                          identifier [get] (59:35-59:38)
                        argument_list [('topic', '')] (59:38-59:51)
                          ( [(] (59:38-59:39)
                          string ['topic'] (59:39-59:46)
                            string_start ['] (59:39-59:40)
                            string_content [topic] (59:40-59:45)
                            string_end ['] (59:45-59:46)
                          , [,] (59:46-59:47)
                          string [''] (59:48-59:50)
                            string_start ['] (59:48-59:49)
                            string_end ['] (59:49-59:50)
                          ) [)] (59:50-59:51)
                    , [,] (59:51-59:52)
                    pair ['scraped_at': datetime.now().isoformat()] (60:16-60:56)
                      string ['scraped_at'] (60:16-60:28)
                        string_start ['] (60:16-60:17)
                        string_content [scraped_at] (60:17-60:27)
                        string_end ['] (60:27-60:28)
                      : [:] (60:28-60:29)
                      call [datetime.now().isoformat()] (60:30-60:56)
                        attribute [datetime.now().isoformat] (60:30-60:54)
                          call [datetime.now()] (60:30-60:44)
                            attribute [datetime.now] (60:30-60:42)
                              identifier [datetime] (60:30-60:38)
                              . [.] (60:38-60:39)
                              identifier [now] (60:39-60:42)
                            argument_list [()] (60:42-60:44)
                              ( [(] (60:42-60:43)
                              ) [)] (60:43-60:44)
                          . [.] (60:44-60:45)
                          identifier [isoformat] (60:45-60:54)
                        argument_list [()] (60:54-60:56)
                          ( [(] (60:54-60:55)
                          ) [)] (60:55-60:56)
                    } [}] (61:12-61:13)
              comment [# Add truncated comments] (63:12-63:36)
              if_statement [if 'comments' in post_info:
                metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]] (64:12-72:17)
                if [if] (64:12-64:14)
                comparison_operator ['comments' in post_info] (64:15-64:38)
                  string ['comments'] (64:15-64:25)
                    string_start ['] (64:15-64:16)
                    string_content [comments] (64:16-64:24)
                    string_end ['] (64:24-64:25)
                  in [in] (64:26-64:28)
                  identifier [post_info] (64:29-64:38)
                : [:] (64:38-64:39)
                block [metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]] (65:16-72:17)
                  expression_statement [metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]] (65:16-72:17)
                    assignment [metadata['comments'] = [
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]] (65:16-72:17)
                      subscript [metadata['comments']] (65:16-65:36)
                        identifier [metadata] (65:16-65:24)
                        [ [[] (65:24-65:25)
                        string ['comments'] (65:25-65:35)
                          string_start ['] (65:25-65:26)
                          string_content [comments] (65:26-65:34)
                          string_end ['] (65:34-65:35)
                        ] []] (65:35-65:36)
                      = [=] (65:37-65:38)
                      list_comprehension [[
                    {
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }
                    for c in post_info['comments'][:5]  # Limit to top 5 comments
                ]] (65:39-72:17)
                        [ [[] (65:39-65:40)
                        dictionary [{
                        'body': c.get('body', '')[:500],
                        'author': c.get('author', '')[:100],
                        'score': c.get('score', 0)
                    }] (66:20-70:21)
                          { [{] (66:20-66:21)
                          pair ['body': c.get('body', '')[:500]] (67:24-67:55)
                            string ['body'] (67:24-67:30)
                              string_start ['] (67:24-67:25)
                              string_content [body] (67:25-67:29)
                              string_end ['] (67:29-67:30)
                            : [:] (67:30-67:31)
                            subscript [c.get('body', '')[:500]] (67:32-67:55)
                              call [c.get('body', '')] (67:32-67:49)
                                attribute [c.get] (67:32-67:37)
                                  identifier [c] (67:32-67:33)
                                  . [.] (67:33-67:34)
                                  identifier [get] (67:34-67:37)
                                argument_list [('body', '')] (67:37-67:49)
                                  ( [(] (67:37-67:38)
                                  string ['body'] (67:38-67:44)
                                    string_start ['] (67:38-67:39)
                                    string_content [body] (67:39-67:43)
                                    string_end ['] (67:43-67:44)
                                  , [,] (67:44-67:45)
                                  string [''] (67:46-67:48)
                                    string_start ['] (67:46-67:47)
                                    string_end ['] (67:47-67:48)
                                  ) [)] (67:48-67:49)
                              [ [[] (67:49-67:50)
                              slice [:500] (67:50-67:54)
                                : [:] (67:50-67:51)
                                integer [500] (67:51-67:54)
                              ] []] (67:54-67:55)
                          , [,] (67:55-67:56)
                          pair ['author': c.get('author', '')[:100]] (68:24-68:59)
                            string ['author'] (68:24-68:32)
                              string_start ['] (68:24-68:25)
                              string_content [author] (68:25-68:31)
                              string_end ['] (68:31-68:32)
                            : [:] (68:32-68:33)
                            subscript [c.get('author', '')[:100]] (68:34-68:59)
                              call [c.get('author', '')] (68:34-68:53)
                                attribute [c.get] (68:34-68:39)
                                  identifier [c] (68:34-68:35)
                                  . [.] (68:35-68:36)
                                  identifier [get] (68:36-68:39)
                                argument_list [('author', '')] (68:39-68:53)
                                  ( [(] (68:39-68:40)
                                  string ['author'] (68:40-68:48)
                                    string_start ['] (68:40-68:41)
                                    string_content [author] (68:41-68:47)
                                    string_end ['] (68:47-68:48)
                                  , [,] (68:48-68:49)
                                  string [''] (68:50-68:52)
                                    string_start ['] (68:50-68:51)
                                    string_end ['] (68:51-68:52)
                                  ) [)] (68:52-68:53)
                              [ [[] (68:53-68:54)
                              slice [:100] (68:54-68:58)
                                : [:] (68:54-68:55)
                                integer [100] (68:55-68:58)
                              ] []] (68:58-68:59)
                          , [,] (68:59-68:60)
                          pair ['score': c.get('score', 0)] (69:24-69:50)
                            string ['score'] (69:24-69:31)
                              string_start ['] (69:24-69:25)
                              string_content [score] (69:25-69:30)
                              string_end ['] (69:30-69:31)
                            : [:] (69:31-69:32)
                            call [c.get('score', 0)] (69:33-69:50)
                              attribute [c.get] (69:33-69:38)
                                identifier [c] (69:33-69:34)
                                . [.] (69:34-69:35)
                                identifier [get] (69:35-69:38)
                              argument_list [('score', 0)] (69:38-69:50)
                                ( [(] (69:38-69:39)
                                string ['score'] (69:39-69:46)
                                  string_start ['] (69:39-69:40)
                                  string_content [score] (69:40-69:45)
                                  string_end ['] (69:45-69:46)
                                , [,] (69:46-69:47)
                                integer [0] (69:48-69:49)
                                ) [)] (69:49-69:50)
                          } [}] (70:20-70:21)
                        for_in_clause [for c in post_info['comments'][:5]] (71:20-71:54)
                          for [for] (71:20-71:23)
                          identifier [c] (71:24-71:25)
                          in [in] (71:26-71:28)
                          subscript [post_info['comments'][:5]] (71:29-71:54)
                            subscript [post_info['comments']] (71:29-71:50)
                              identifier [post_info] (71:29-71:38)
                              [ [[] (71:38-71:39)
                              string ['comments'] (71:39-71:49)
                                string_start ['] (71:39-71:40)
                                string_content [comments] (71:40-71:48)
                                string_end ['] (71:48-71:49)
                              ] []] (71:49-71:50)
                            [ [[] (71:50-71:51)
                            slice [:5] (71:51-71:53)
                              : [:] (71:51-71:52)
                              integer [5] (71:52-71:53)
                            ] []] (71:53-71:54)
                        comment [# Limit to top 5 comments] (71:56-71:81)
                        ] []] (72:16-72:17)
              return_statement [return {
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }] (74:12-78:13)
                return [return] (74:12-74:18)
                dictionary [{
                'content': text_content,
                'metadata': metadata,
                '$vector': self.create_vector(text_content)
            }] (74:19-78:13)
                  { [{] (74:19-74:20)
                  pair ['content': text_content] (75:16-75:39)
                    string ['content'] (75:16-75:25)
                      string_start ['] (75:16-75:17)
                      string_content [content] (75:17-75:24)
                      string_end ['] (75:24-75:25)
                    : [:] (75:25-75:26)
                    identifier [text_content] (75:27-75:39)
                  , [,] (75:39-75:40)
                  pair ['metadata': metadata] (76:16-76:36)
                    string ['metadata'] (76:16-76:26)
                      string_start ['] (76:16-76:17)
                      string_content [metadata] (76:17-76:25)
                      string_end ['] (76:25-76:26)
                    : [:] (76:26-76:27)
                    identifier [metadata] (76:28-76:36)
                  , [,] (76:36-76:37)
                  pair ['$vector': self.create_vector(text_content)] (77:16-77:59)
                    string ['$vector'] (77:16-77:25)
                      string_start ['] (77:16-77:17)
                      string_content [$vector] (77:17-77:24)
                      string_end ['] (77:24-77:25)
                    : [:] (77:25-77:26)
                    call [self.create_vector(text_content)] (77:27-77:59)
                      attribute [self.create_vector] (77:27-77:45)
                        identifier [self] (77:27-77:31)
                        . [.] (77:31-77:32)
                        identifier [create_vector] (77:32-77:45)
                      argument_list [(text_content)] (77:45-77:59)
                        ( [(] (77:45-77:46)
                        identifier [text_content] (77:46-77:58)
                        ) [)] (77:58-77:59)
                  } [}] (78:12-78:13)
            except_clause [except Exception as e:
            print(f"Error preparing document: {e}")
            return None] (79:8-81:23)
              except [except] (79:8-79:14)
              as_pattern [Exception as e] (79:15-79:29)
                identifier [Exception] (79:15-79:24)
                as [as] (79:25-79:27)
                as_pattern_target [e] (79:28-79:29)
                  identifier [e] (79:28-79:29)
              : [:] (79:29-79:30)
              block [print(f"Error preparing document: {e}")
            return None] (80:12-81:23)
                expression_statement [print(f"Error preparing document: {e}")] (80:12-80:51)
                  call [print(f"Error preparing document: {e}")] (80:12-80:51)
                    identifier [print] (80:12-80:17)
                    argument_list [(f"Error preparing document: {e}")] (80:17-80:51)
                      ( [(] (80:17-80:18)
                      string [f"Error preparing document: {e}"] (80:18-80:50)
                        string_start [f"] (80:18-80:20)
                        string_content [Error preparing document: ] (80:20-80:46)
                        interpolation [{e}] (80:46-80:49)
                          { [{] (80:46-80:47)
                          identifier [e] (80:47-80:48)
                          } [}] (80:48-80:49)
                        string_end ["] (80:49-80:50)
                      ) [)] (80:50-80:51)
                return_statement [return None] (81:12-81:23)
                  return [return] (81:12-81:18)
                  none [None] (81:19-81:23)
      function_definition [def search_and_store_posts(self, topic: str, limit: int = 5) -> dict:
        """Search for Reddit posts and store them in AstraDB."""
        results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }

        try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results] (83:4-147:26)
        def [def] (83:4-83:7)
        identifier [search_and_store_posts] (83:8-83:30)
        parameters [(self, topic: str, limit: int = 5)] (83:30-83:64)
          ( [(] (83:30-83:31)
          identifier [self] (83:31-83:35)
          , [,] (83:35-83:36)
          typed_parameter [topic: str] (83:37-83:47)
            identifier [topic] (83:37-83:42)
            : [:] (83:42-83:43)
            type [str] (83:44-83:47)
              identifier [str] (83:44-83:47)
          , [,] (83:47-83:48)
          typed_default_parameter [limit: int = 5] (83:49-83:63)
            identifier [limit] (83:49-83:54)
            : [:] (83:54-83:55)
            type [int] (83:56-83:59)
              identifier [int] (83:56-83:59)
            = [=] (83:60-83:61)
            integer [5] (83:62-83:63)
          ) [)] (83:63-83:64)
        -> [->] (83:65-83:67)
        type [dict] (83:68-83:72)
          identifier [dict] (83:68-83:72)
        : [:] (83:72-83:73)
        block ["""Search for Reddit posts and store them in AstraDB."""
        results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }

        try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results] (84:8-147:26)
          expression_statement ["""Search for Reddit posts and store them in AstraDB."""] (84:8-84:64)
            string ["""Search for Reddit posts and store them in AstraDB."""] (84:8-84:64)
              string_start ["""] (84:8-84:11)
              string_content [Search for Reddit posts and store them in AstraDB.] (84:11-84:61)
              string_end ["""] (84:61-84:64)
          expression_statement [results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }] (85:8-90:9)
            assignment [results = {
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }] (85:8-90:9)
              identifier [results] (85:8-85:15)
              = [=] (85:16-85:17)
              dictionary [{
            'posts_found': 0,
            'posts_stored': 0,
            'posts_failed': 0,
            'file_path': None
        }] (85:18-90:9)
                { [{] (85:18-85:19)
                pair ['posts_found': 0] (86:12-86:28)
                  string ['posts_found'] (86:12-86:25)
                    string_start ['] (86:12-86:13)
                    string_content [posts_found] (86:13-86:24)
                    string_end ['] (86:24-86:25)
                  : [:] (86:25-86:26)
                  integer [0] (86:27-86:28)
                , [,] (86:28-86:29)
                pair ['posts_stored': 0] (87:12-87:29)
                  string ['posts_stored'] (87:12-87:26)
                    string_start ['] (87:12-87:13)
                    string_content [posts_stored] (87:13-87:25)
                    string_end ['] (87:25-87:26)
                  : [:] (87:26-87:27)
                  integer [0] (87:28-87:29)
                , [,] (87:29-87:30)
                pair ['posts_failed': 0] (88:12-88:29)
                  string ['posts_failed'] (88:12-88:26)
                    string_start ['] (88:12-88:13)
                    string_content [posts_failed] (88:13-88:25)
                    string_end ['] (88:25-88:26)
                  : [:] (88:26-88:27)
                  integer [0] (88:28-88:29)
                , [,] (88:29-88:30)
                pair ['file_path': None] (89:12-89:29)
                  string ['file_path'] (89:12-89:23)
                    string_start ['] (89:12-89:13)
                    string_content [file_path] (89:13-89:22)
                    string_end ['] (89:22-89:23)
                  : [:] (89:23-89:24)
                  none [None] (89:25-89:29)
                } [}] (90:8-90:9)
          try_statement [try:
            # Generate subreddits based on topic
            subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results

        except Exception as e:
            print(f"Error in search and store process: {e}")
            return results] (92:8-147:26)
            try [try] (92:8-92:11)
            : [:] (92:11-92:12)
            comment [# Generate subreddits based on topic] (93:12-93:48)
            block [subreddits = self._generate_subreddits_for_topic(topic)
            all_posts = []

            # Search each subreddit
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")

            # Save local copy
            if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)

            return results] (94:12-143:26)
              expression_statement [subreddits = self._generate_subreddits_for_topic(topic)] (94:12-94:67)
                assignment [subreddits = self._generate_subreddits_for_topic(topic)] (94:12-94:67)
                  identifier [subreddits] (94:12-94:22)
                  = [=] (94:23-94:24)
                  call [self._generate_subreddits_for_topic(topic)] (94:25-94:67)
                    attribute [self._generate_subreddits_for_topic] (94:25-94:60)
                      identifier [self] (94:25-94:29)
                      . [.] (94:29-94:30)
                      identifier [_generate_subreddits_for_topic] (94:30-94:60)
                    argument_list [(topic)] (94:60-94:67)
                      ( [(] (94:60-94:61)
                      identifier [topic] (94:61-94:66)
                      ) [)] (94:66-94:67)
              expression_statement [all_posts = []] (95:12-95:26)
                assignment [all_posts = []] (95:12-95:26)
                  identifier [all_posts] (95:12-95:21)
                  = [=] (95:22-95:23)
                  list [[]] (95:24-95:26)
                    [ [[] (95:24-95:25)
                    ] []] (95:25-95:26)
              comment [# Search each subreddit] (97:12-97:35)
              for_statement [for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (98:12-137:89)
                for [for] (98:12-98:15)
                identifier [subreddit_name] (98:16-98:30)
                in [in] (98:31-98:33)
                identifier [subreddits] (98:34-98:44)
                : [:] (98:44-98:45)
                block [try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (99:16-137:89)
                  try_statement [try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")

                except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (99:16-137:89)
                    try [try] (99:16-99:19)
                    : [:] (99:19-99:20)
                    block [subreddit = self.reddit.subreddit(subreddit_name)
                    top_posts = list(subreddit.top(time_filter='week', limit=limit))
                    results['posts_found'] += len(top_posts)

                    for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (100:20-134:71)
                      expression_statement [subreddit = self.reddit.subreddit(subreddit_name)] (100:20-100:69)
                        assignment [subreddit = self.reddit.subreddit(subreddit_name)] (100:20-100:69)
                          identifier [subreddit] (100:20-100:29)
                          = [=] (100:30-100:31)
                          call [self.reddit.subreddit(subreddit_name)] (100:32-100:69)
                            attribute [self.reddit.subreddit] (100:32-100:53)
                              attribute [self.reddit] (100:32-100:43)
                                identifier [self] (100:32-100:36)
                                . [.] (100:36-100:37)
                                identifier [reddit] (100:37-100:43)
                              . [.] (100:43-100:44)
                              identifier [subreddit] (100:44-100:53)
                            argument_list [(subreddit_name)] (100:53-100:69)
                              ( [(] (100:53-100:54)
                              identifier [subreddit_name] (100:54-100:68)
                              ) [)] (100:68-100:69)
                      expression_statement [top_posts = list(subreddit.top(time_filter='week', limit=limit))] (101:20-101:84)
                        assignment [top_posts = list(subreddit.top(time_filter='week', limit=limit))] (101:20-101:84)
                          identifier [top_posts] (101:20-101:29)
                          = [=] (101:30-101:31)
                          call [list(subreddit.top(time_filter='week', limit=limit))] (101:32-101:84)
                            identifier [list] (101:32-101:36)
                            argument_list [(subreddit.top(time_filter='week', limit=limit))] (101:36-101:84)
                              ( [(] (101:36-101:37)
                              call [subreddit.top(time_filter='week', limit=limit)] (101:37-101:83)
                                attribute [subreddit.top] (101:37-101:50)
                                  identifier [subreddit] (101:37-101:46)
                                  . [.] (101:46-101:47)
                                  identifier [top] (101:47-101:50)
                                argument_list [(time_filter='week', limit=limit)] (101:50-101:83)
                                  ( [(] (101:50-101:51)
                                  keyword_argument [time_filter='week'] (101:51-101:69)
                                    identifier [time_filter] (101:51-101:62)
                                    = [=] (101:62-101:63)
                                    string ['week'] (101:63-101:69)
                                      string_start ['] (101:63-101:64)
                                      string_content [week] (101:64-101:68)
                                      string_end ['] (101:68-101:69)
                                  , [,] (101:69-101:70)
                                  keyword_argument [limit=limit] (101:71-101:82)
                                    identifier [limit] (101:71-101:76)
                                    = [=] (101:76-101:77)
                                    identifier [limit] (101:77-101:82)
                                  ) [)] (101:82-101:83)
                              ) [)] (101:83-101:84)
                      expression_statement [results['posts_found'] += len(top_posts)] (102:20-102:60)
                        augmented_assignment [results['posts_found'] += len(top_posts)] (102:20-102:60)
                          subscript [results['posts_found']] (102:20-102:42)
                            identifier [results] (102:20-102:27)
                            [ [[] (102:27-102:28)
                            string ['posts_found'] (102:28-102:41)
                              string_start ['] (102:28-102:29)
                              string_content [posts_found] (102:29-102:40)
                              string_end ['] (102:40-102:41)
                            ] []] (102:41-102:42)
                          += [+=] (102:43-102:45)
                          call [len(top_posts)] (102:46-102:60)
                            identifier [len] (102:46-102:49)
                            argument_list [(top_posts)] (102:49-102:60)
                              ( [(] (102:49-102:50)
                              identifier [top_posts] (102:50-102:59)
                              ) [)] (102:59-102:60)
                      for_statement [for submission in top_posts:
                        try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (104:20-134:71)
                        for [for] (104:20-104:23)
                        identifier [submission] (104:24-104:34)
                        in [in] (104:35-104:37)
                        identifier [top_posts] (104:38-104:47)
                        : [:] (104:47-104:48)
                        block [try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (105:24-134:71)
                          try_statement [try:
                            submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")

                        except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (105:24-134:71)
                            try [try] (105:24-105:27)
                            : [:] (105:27-105:28)
                            block [submission.comments.replace_more(limit=None)

                            post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }

                            # Prepare and store document
                            document = self.prepare_document(post_info)
                            if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (106:28-130:92)
                              expression_statement [submission.comments.replace_more(limit=None)] (106:28-106:72)
                                call [submission.comments.replace_more(limit=None)] (106:28-106:72)
                                  attribute [submission.comments.replace_more] (106:28-106:60)
                                    attribute [submission.comments] (106:28-106:47)
                                      identifier [submission] (106:28-106:38)
                                      . [.] (106:38-106:39)
                                      identifier [comments] (106:39-106:47)
                                    . [.] (106:47-106:48)
                                    identifier [replace_more] (106:48-106:60)
                                  argument_list [(limit=None)] (106:60-106:72)
                                    ( [(] (106:60-106:61)
                                    keyword_argument [limit=None] (106:61-106:71)
                                      identifier [limit] (106:61-106:66)
                                      = [=] (106:66-106:67)
                                      none [None] (106:67-106:71)
                                    ) [)] (106:71-106:72)
                              expression_statement [post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }] (108:28-119:29)
                                assignment [post_info = {
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }] (108:28-119:29)
                                  identifier [post_info] (108:28-108:37)
                                  = [=] (108:38-108:39)
                                  dictionary [{
                                'title': submission.title,
                                'url': submission.url,
                                'subreddit': subreddit_name,
                                'topic': topic,
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created_utc': submission.created_utc,
                                'selftext': submission.selftext,
                                'comments': self._parse_comments_recursively(submission.comments)
                            }] (108:40-119:29)
                                    { [{] (108:40-108:41)
                                    pair ['title': submission.title] (109:32-109:57)
                                      string ['title'] (109:32-109:39)
                                        string_start ['] (109:32-109:33)
                                        string_content [title] (109:33-109:38)
                                        string_end ['] (109:38-109:39)
                                      : [:] (109:39-109:40)
                                      attribute [submission.title] (109:41-109:57)
                                        identifier [submission] (109:41-109:51)
                                        . [.] (109:51-109:52)
                                        identifier [title] (109:52-109:57)
                                    , [,] (109:57-109:58)
                                    pair ['url': submission.url] (110:32-110:53)
                                      string ['url'] (110:32-110:37)
                                        string_start ['] (110:32-110:33)
                                        string_content [url] (110:33-110:36)
                                        string_end ['] (110:36-110:37)
                                      : [:] (110:37-110:38)
                                      attribute [submission.url] (110:39-110:53)
                                        identifier [submission] (110:39-110:49)
                                        . [.] (110:49-110:50)
                                        identifier [url] (110:50-110:53)
                                    , [,] (110:53-110:54)
                                    pair ['subreddit': subreddit_name] (111:32-111:59)
                                      string ['subreddit'] (111:32-111:43)
                                        string_start ['] (111:32-111:33)
                                        string_content [subreddit] (111:33-111:42)
                                        string_end ['] (111:42-111:43)
                                      : [:] (111:43-111:44)
                                      identifier [subreddit_name] (111:45-111:59)
                                    , [,] (111:59-111:60)
                                    pair ['topic': topic] (112:32-112:46)
                                      string ['topic'] (112:32-112:39)
                                        string_start ['] (112:32-112:33)
                                        string_content [topic] (112:33-112:38)
                                        string_end ['] (112:38-112:39)
                                      : [:] (112:39-112:40)
                                      identifier [topic] (112:41-112:46)
                                    , [,] (112:46-112:47)
                                    pair ['author': str(submission.author)] (113:32-113:64)
                                      string ['author'] (113:32-113:40)
                                        string_start ['] (113:32-113:33)
                                        string_content [author] (113:33-113:39)
                                        string_end ['] (113:39-113:40)
                                      : [:] (113:40-113:41)
                                      call [str(submission.author)] (113:42-113:64)
                                        identifier [str] (113:42-113:45)
                                        argument_list [(submission.author)] (113:45-113:64)
                                          ( [(] (113:45-113:46)
                                          attribute [submission.author] (113:46-113:63)
                                            identifier [submission] (113:46-113:56)
                                            . [.] (113:56-113:57)
                                            identifier [author] (113:57-113:63)
                                          ) [)] (113:63-113:64)
                                    , [,] (113:64-113:65)
                                    pair ['score': submission.score] (114:32-114:57)
                                      string ['score'] (114:32-114:39)
                                        string_start ['] (114:32-114:33)
                                        string_content [score] (114:33-114:38)
                                        string_end ['] (114:38-114:39)
                                      : [:] (114:39-114:40)
                                      attribute [submission.score] (114:41-114:57)
                                        identifier [submission] (114:41-114:51)
                                        . [.] (114:51-114:52)
                                        identifier [score] (114:52-114:57)
                                    , [,] (114:57-114:58)
                                    pair ['num_comments': submission.num_comments] (115:32-115:71)
                                      string ['num_comments'] (115:32-115:46)
                                        string_start ['] (115:32-115:33)
                                        string_content [num_comments] (115:33-115:45)
                                        string_end ['] (115:45-115:46)
                                      : [:] (115:46-115:47)
                                      attribute [submission.num_comments] (115:48-115:71)
                                        identifier [submission] (115:48-115:58)
                                        . [.] (115:58-115:59)
                                        identifier [num_comments] (115:59-115:71)
                                    , [,] (115:71-115:72)
                                    pair ['created_utc': submission.created_utc] (116:32-116:69)
                                      string ['created_utc'] (116:32-116:45)
                                        string_start ['] (116:32-116:33)
                                        string_content [created_utc] (116:33-116:44)
                                        string_end ['] (116:44-116:45)
                                      : [:] (116:45-116:46)
                                      attribute [submission.created_utc] (116:47-116:69)
                                        identifier [submission] (116:47-116:57)
                                        . [.] (116:57-116:58)
                                        identifier [created_utc] (116:58-116:69)
                                    , [,] (116:69-116:70)
                                    pair ['selftext': submission.selftext] (117:32-117:63)
                                      string ['selftext'] (117:32-117:42)
                                        string_start ['] (117:32-117:33)
                                        string_content [selftext] (117:33-117:41)
                                        string_end ['] (117:41-117:42)
                                      : [:] (117:42-117:43)
                                      attribute [submission.selftext] (117:44-117:63)
                                        identifier [submission] (117:44-117:54)
                                        . [.] (117:54-117:55)
                                        identifier [selftext] (117:55-117:63)
                                    , [,] (117:63-117:64)
                                    pair ['comments': self._parse_comments_recursively(submission.comments)] (118:32-118:97)
                                      string ['comments'] (118:32-118:42)
                                        string_start ['] (118:32-118:33)
                                        string_content [comments] (118:33-118:41)
                                        string_end ['] (118:41-118:42)
                                      : [:] (118:42-118:43)
                                      call [self._parse_comments_recursively(submission.comments)] (118:44-118:97)
                                        attribute [self._parse_comments_recursively] (118:44-118:76)
                                          identifier [self] (118:44-118:48)
                                          . [.] (118:48-118:49)
                                          identifier [_parse_comments_recursively] (118:49-118:76)
                                        argument_list [(submission.comments)] (118:76-118:97)
                                          ( [(] (118:76-118:77)
                                          attribute [submission.comments] (118:77-118:96)
                                            identifier [submission] (118:77-118:87)
                                            . [.] (118:87-118:88)
                                            identifier [comments] (118:88-118:96)
                                          ) [)] (118:96-118:97)
                                    } [}] (119:28-119:29)
                              comment [# Prepare and store document] (121:28-121:56)
                              expression_statement [document = self.prepare_document(post_info)] (122:28-122:71)
                                assignment [document = self.prepare_document(post_info)] (122:28-122:71)
                                  identifier [document] (122:28-122:36)
                                  = [=] (122:37-122:38)
                                  call [self.prepare_document(post_info)] (122:39-122:71)
                                    attribute [self.prepare_document] (122:39-122:60)
                                      identifier [self] (122:39-122:43)
                                      . [.] (122:43-122:44)
                                      identifier [prepare_document] (122:44-122:60)
                                    argument_list [(post_info)] (122:60-122:71)
                                      ( [(] (122:60-122:61)
                                      identifier [post_info] (122:61-122:70)
                                      ) [)] (122:70-122:71)
                              if_statement [if document:
                                try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (123:28-130:92)
                                if [if] (123:28-123:30)
                                identifier [document] (123:31-123:39)
                                : [:] (123:39-123:40)
                                block [try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (124:32-130:92)
                                  try_statement [try:
                                    self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)
                                except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (124:32-130:92)
                                    try [try] (124:32-124:35)
                                    : [:] (124:35-124:36)
                                    block [self.collection.insert_one(document)
                                    results['posts_stored'] += 1
                                    all_posts.append(post_info)] (125:36-127:63)
                                      expression_statement [self.collection.insert_one(document)] (125:36-125:72)
                                        call [self.collection.insert_one(document)] (125:36-125:72)
                                          attribute [self.collection.insert_one] (125:36-125:62)
                                            attribute [self.collection] (125:36-125:51)
                                              identifier [self] (125:36-125:40)
                                              . [.] (125:40-125:41)
                                              identifier [collection] (125:41-125:51)
                                            . [.] (125:51-125:52)
                                            identifier [insert_one] (125:52-125:62)
                                          argument_list [(document)] (125:62-125:72)
                                            ( [(] (125:62-125:63)
                                            identifier [document] (125:63-125:71)
                                            ) [)] (125:71-125:72)
                                      expression_statement [results['posts_stored'] += 1] (126:36-126:64)
                                        augmented_assignment [results['posts_stored'] += 1] (126:36-126:64)
                                          subscript [results['posts_stored']] (126:36-126:59)
                                            identifier [results] (126:36-126:43)
                                            [ [[] (126:43-126:44)
                                            string ['posts_stored'] (126:44-126:58)
                                              string_start ['] (126:44-126:45)
                                              string_content [posts_stored] (126:45-126:57)
                                              string_end ['] (126:57-126:58)
                                            ] []] (126:58-126:59)
                                          += [+=] (126:60-126:62)
                                          integer [1] (126:63-126:64)
                                      expression_statement [all_posts.append(post_info)] (127:36-127:63)
                                        call [all_posts.append(post_info)] (127:36-127:63)
                                          attribute [all_posts.append] (127:36-127:52)
                                            identifier [all_posts] (127:36-127:45)
                                            . [.] (127:45-127:46)
                                            identifier [append] (127:46-127:52)
                                          argument_list [(post_info)] (127:52-127:63)
                                            ( [(] (127:52-127:53)
                                            identifier [post_info] (127:53-127:62)
                                            ) [)] (127:62-127:63)
                                    except_clause [except Exception as store_error:
                                    results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (128:32-130:92)
                                      except [except] (128:32-128:38)
                                      as_pattern [Exception as store_error] (128:39-128:63)
                                        identifier [Exception] (128:39-128:48)
                                        as [as] (128:49-128:51)
                                        as_pattern_target [store_error] (128:52-128:63)
                                          identifier [store_error] (128:52-128:63)
                                      : [:] (128:63-128:64)
                                      block [results['posts_failed'] += 1
                                    print(f"Failed to store post in AstraDB: {store_error}")] (129:36-130:92)
                                        expression_statement [results['posts_failed'] += 1] (129:36-129:64)
                                          augmented_assignment [results['posts_failed'] += 1] (129:36-129:64)
                                            subscript [results['posts_failed']] (129:36-129:59)
                                              identifier [results] (129:36-129:43)
                                              [ [[] (129:43-129:44)
                                              string ['posts_failed'] (129:44-129:58)
                                                string_start ['] (129:44-129:45)
                                                string_content [posts_failed] (129:45-129:57)
                                                string_end ['] (129:57-129:58)
                                              ] []] (129:58-129:59)
                                            += [+=] (129:60-129:62)
                                            integer [1] (129:63-129:64)
                                        expression_statement [print(f"Failed to store post in AstraDB: {store_error}")] (130:36-130:92)
                                          call [print(f"Failed to store post in AstraDB: {store_error}")] (130:36-130:92)
                                            identifier [print] (130:36-130:41)
                                            argument_list [(f"Failed to store post in AstraDB: {store_error}")] (130:41-130:92)
                                              ( [(] (130:41-130:42)
                                              string [f"Failed to store post in AstraDB: {store_error}"] (130:42-130:91)
                                                string_start [f"] (130:42-130:44)
                                                string_content [Failed to store post in AstraDB: ] (130:44-130:77)
                                                interpolation [{store_error}] (130:77-130:90)
                                                  { [{] (130:77-130:78)
                                                  identifier [store_error] (130:78-130:89)
                                                  } [}] (130:89-130:90)
                                                string_end ["] (130:90-130:91)
                                              ) [)] (130:91-130:92)
                            except_clause [except Exception as post_err:
                            results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (132:24-134:71)
                              except [except] (132:24-132:30)
                              as_pattern [Exception as post_err] (132:31-132:52)
                                identifier [Exception] (132:31-132:40)
                                as [as] (132:41-132:43)
                                as_pattern_target [post_err] (132:44-132:52)
                                  identifier [post_err] (132:44-132:52)
                              : [:] (132:52-132:53)
                              block [results['posts_failed'] += 1
                            print(f"Error processing post: {post_err}")] (133:28-134:71)
                                expression_statement [results['posts_failed'] += 1] (133:28-133:56)
                                  augmented_assignment [results['posts_failed'] += 1] (133:28-133:56)
                                    subscript [results['posts_failed']] (133:28-133:51)
                                      identifier [results] (133:28-133:35)
                                      [ [[] (133:35-133:36)
                                      string ['posts_failed'] (133:36-133:50)
                                        string_start ['] (133:36-133:37)
                                        string_content [posts_failed] (133:37-133:49)
                                        string_end ['] (133:49-133:50)
                                      ] []] (133:50-133:51)
                                    += [+=] (133:52-133:54)
                                    integer [1] (133:55-133:56)
                                expression_statement [print(f"Error processing post: {post_err}")] (134:28-134:71)
                                  call [print(f"Error processing post: {post_err}")] (134:28-134:71)
                                    identifier [print] (134:28-134:33)
                                    argument_list [(f"Error processing post: {post_err}")] (134:33-134:71)
                                      ( [(] (134:33-134:34)
                                      string [f"Error processing post: {post_err}"] (134:34-134:70)
                                        string_start [f"] (134:34-134:36)
                                        string_content [Error processing post: ] (134:36-134:59)
                                        interpolation [{post_err}] (134:59-134:69)
                                          { [{] (134:59-134:60)
                                          identifier [post_err] (134:60-134:68)
                                          } [}] (134:68-134:69)
                                        string_end ["] (134:69-134:70)
                                      ) [)] (134:70-134:71)
                    except_clause [except Exception as subreddit_err:
                    print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (136:16-137:89)
                      except [except] (136:16-136:22)
                      as_pattern [Exception as subreddit_err] (136:23-136:49)
                        identifier [Exception] (136:23-136:32)
                        as [as] (136:33-136:35)
                        as_pattern_target [subreddit_err] (136:36-136:49)
                          identifier [subreddit_err] (136:36-136:49)
                      : [:] (136:49-136:50)
                      block [print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (137:20-137:89)
                        expression_statement [print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (137:20-137:89)
                          call [print(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (137:20-137:89)
                            identifier [print] (137:20-137:25)
                            argument_list [(f"Error accessing subreddit {subreddit_name}: {subreddit_err}")] (137:25-137:89)
                              ( [(] (137:25-137:26)
                              string [f"Error accessing subreddit {subreddit_name}: {subreddit_err}"] (137:26-137:88)
                                string_start [f"] (137:26-137:28)
                                string_content [Error accessing subreddit ] (137:28-137:54)
                                interpolation [{subreddit_name}] (137:54-137:70)
                                  { [{] (137:54-137:55)
                                  identifier [subreddit_name] (137:55-137:69)
                                  } [}] (137:69-137:70)
                                string_content [: ] (137:70-137:72)
                                interpolation [{subreddit_err}] (137:72-137:87)
                                  { [{] (137:72-137:73)
                                  identifier [subreddit_err] (137:73-137:86)
                                  } [}] (137:86-137:87)
                                string_end ["] (137:87-137:88)
                              ) [)] (137:88-137:89)
              comment [# Save local copy] (139:12-139:29)
              if_statement [if all_posts:
                results['file_path'] = self._save_local_copy(all_posts, topic)] (140:12-141:78)
                if [if] (140:12-140:14)
                identifier [all_posts] (140:15-140:24)
                : [:] (140:24-140:25)
                block [results['file_path'] = self._save_local_copy(all_posts, topic)] (141:16-141:78)
                  expression_statement [results['file_path'] = self._save_local_copy(all_posts, topic)] (141:16-141:78)
                    assignment [results['file_path'] = self._save_local_copy(all_posts, topic)] (141:16-141:78)
                      subscript [results['file_path']] (141:16-141:36)
                        identifier [results] (141:16-141:23)
                        [ [[] (141:23-141:24)
                        string ['file_path'] (141:24-141:35)
                          string_start ['] (141:24-141:25)
                          string_content [file_path] (141:25-141:34)
                          string_end ['] (141:34-141:35)
                        ] []] (141:35-141:36)
                      = [=] (141:37-141:38)
                      call [self._save_local_copy(all_posts, topic)] (141:39-141:78)
                        attribute [self._save_local_copy] (141:39-141:60)
                          identifier [self] (141:39-141:43)
                          . [.] (141:43-141:44)
                          identifier [_save_local_copy] (141:44-141:60)
                        argument_list [(all_posts, topic)] (141:60-141:78)
                          ( [(] (141:60-141:61)
                          identifier [all_posts] (141:61-141:70)
                          , [,] (141:70-141:71)
                          identifier [topic] (141:72-141:77)
                          ) [)] (141:77-141:78)
              return_statement [return results] (143:12-143:26)
                return [return] (143:12-143:18)
                identifier [results] (143:19-143:26)
            except_clause [except Exception as e:
            print(f"Error in search and store process: {e}")
            return results] (145:8-147:26)
              except [except] (145:8-145:14)
              as_pattern [Exception as e] (145:15-145:29)
                identifier [Exception] (145:15-145:24)
                as [as] (145:25-145:27)
                as_pattern_target [e] (145:28-145:29)
                  identifier [e] (145:28-145:29)
              : [:] (145:29-145:30)
              block [print(f"Error in search and store process: {e}")
            return results] (146:12-147:26)
                expression_statement [print(f"Error in search and store process: {e}")] (146:12-146:60)
                  call [print(f"Error in search and store process: {e}")] (146:12-146:60)
                    identifier [print] (146:12-146:17)
                    argument_list [(f"Error in search and store process: {e}")] (146:17-146:60)
                      ( [(] (146:17-146:18)
                      string [f"Error in search and store process: {e}"] (146:18-146:59)
                        string_start [f"] (146:18-146:20)
                        string_content [Error in search and store process: ] (146:20-146:55)
                        interpolation [{e}] (146:55-146:58)
                          { [{] (146:55-146:56)
                          identifier [e] (146:56-146:57)
                          } [}] (146:57-146:58)
                        string_end ["] (146:58-146:59)
                      ) [)] (146:59-146:60)
                return_statement [return results] (147:12-147:26)
                  return [return] (147:12-147:18)
                  identifier [results] (147:19-147:26)
      function_definition [def _generate_subreddits_for_topic(self, topic: str) -> list:
        """Generate relevant subreddits for a topic."""
        topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }

        topic_lower = topic.lower()
        return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])] (149:4-161:81)
        def [def] (149:4-149:7)
        identifier [_generate_subreddits_for_topic] (149:8-149:38)
        parameters [(self, topic: str)] (149:38-149:56)
          ( [(] (149:38-149:39)
          identifier [self] (149:39-149:43)
          , [,] (149:43-149:44)
          typed_parameter [topic: str] (149:45-149:55)
            identifier [topic] (149:45-149:50)
            : [:] (149:50-149:51)
            type [str] (149:52-149:55)
              identifier [str] (149:52-149:55)
          ) [)] (149:55-149:56)
        -> [->] (149:57-149:59)
        type [list] (149:60-149:64)
          identifier [list] (149:60-149:64)
        : [:] (149:64-149:65)
        block ["""Generate relevant subreddits for a topic."""
        topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }

        topic_lower = topic.lower()
        return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])] (150:8-161:81)
          expression_statement ["""Generate relevant subreddits for a topic."""] (150:8-150:55)
            string ["""Generate relevant subreddits for a topic."""] (150:8-150:55)
              string_start ["""] (150:8-150:11)
              string_content [Generate relevant subreddits for a topic.] (150:11-150:52)
              string_end ["""] (150:52-150:55)
          expression_statement [topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }] (151:8-158:9)
            assignment [topic_subreddit_map = {
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }] (151:8-158:9)
              identifier [topic_subreddit_map] (151:8-151:27)
              = [=] (151:28-151:29)
              dictionary [{
            'coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine'],
            'programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding'],
            'technology': ['technology', 'gadgets', 'tech'],
            'science': ['science', 'space', 'physics', 'biology'],
            'tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro'],
            'ram': ['hardware', 'computer', 'buildapc', 'techsupport']
        }] (151:30-158:9)
                { [{] (151:30-151:31)
                pair ['coffee': ['Coffee', 'Coffee_Shop', 'espresso', 'caffeine']] (152:12-152:71)
                  string ['coffee'] (152:12-152:20)
                    string_start ['] (152:12-152:13)
                    string_content [coffee] (152:13-152:19)
                    string_end ['] (152:19-152:20)
                  : [:] (152:20-152:21)
                  list [['Coffee', 'Coffee_Shop', 'espresso', 'caffeine']] (152:22-152:71)
                    [ [[] (152:22-152:23)
                    string ['Coffee'] (152:23-152:31)
                      string_start ['] (152:23-152:24)
                      string_content [Coffee] (152:24-152:30)
                      string_end ['] (152:30-152:31)
                    , [,] (152:31-152:32)
                    string ['Coffee_Shop'] (152:33-152:46)
                      string_start ['] (152:33-152:34)
                      string_content [Coffee_Shop] (152:34-152:45)
                      string_end ['] (152:45-152:46)
                    , [,] (152:46-152:47)
                    string ['espresso'] (152:48-152:58)
                      string_start ['] (152:48-152:49)
                      string_content [espresso] (152:49-152:57)
                      string_end ['] (152:57-152:58)
                    , [,] (152:58-152:59)
                    string ['caffeine'] (152:60-152:70)
                      string_start ['] (152:60-152:61)
                      string_content [caffeine] (152:61-152:69)
                      string_end ['] (152:69-152:70)
                    ] []] (152:70-152:71)
                , [,] (152:71-152:72)
                pair ['programming': ['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding']] (153:12-153:89)
                  string ['programming'] (153:12-153:25)
                    string_start ['] (153:12-153:13)
                    string_content [programming] (153:13-153:24)
                    string_end ['] (153:24-153:25)
                  : [:] (153:25-153:26)
                  list [['Python', 'ProgrammingBuddies', 'learnprogramming', 'coding']] (153:27-153:89)
                    [ [[] (153:27-153:28)
                    string ['Python'] (153:28-153:36)
                      string_start ['] (153:28-153:29)
                      string_content [Python] (153:29-153:35)
                      string_end ['] (153:35-153:36)
                    , [,] (153:36-153:37)
                    string ['ProgrammingBuddies'] (153:38-153:58)
                      string_start ['] (153:38-153:39)
                      string_content [ProgrammingBuddies] (153:39-153:57)
                      string_end ['] (153:57-153:58)
                    , [,] (153:58-153:59)
                    string ['learnprogramming'] (153:60-153:78)
                      string_start ['] (153:60-153:61)
                      string_content [learnprogramming] (153:61-153:77)
                      string_end ['] (153:77-153:78)
                    , [,] (153:78-153:79)
                    string ['coding'] (153:80-153:88)
                      string_start ['] (153:80-153:81)
                      string_content [coding] (153:81-153:87)
                      string_end ['] (153:87-153:88)
                    ] []] (153:88-153:89)
                , [,] (153:89-153:90)
                pair ['technology': ['technology', 'gadgets', 'tech']] (154:12-154:59)
                  string ['technology'] (154:12-154:24)
                    string_start ['] (154:12-154:13)
                    string_content [technology] (154:13-154:23)
                    string_end ['] (154:23-154:24)
                  : [:] (154:24-154:25)
                  list [['technology', 'gadgets', 'tech']] (154:26-154:59)
                    [ [[] (154:26-154:27)
                    string ['technology'] (154:27-154:39)
                      string_start ['] (154:27-154:28)
                      string_content [technology] (154:28-154:38)
                      string_end ['] (154:38-154:39)
                    , [,] (154:39-154:40)
                    string ['gadgets'] (154:41-154:50)
                      string_start ['] (154:41-154:42)
                      string_content [gadgets] (154:42-154:49)
                      string_end ['] (154:49-154:50)
                    , [,] (154:50-154:51)
                    string ['tech'] (154:52-154:58)
                      string_start ['] (154:52-154:53)
                      string_content [tech] (154:53-154:57)
                      string_end ['] (154:57-154:58)
                    ] []] (154:58-154:59)
                , [,] (154:59-154:60)
                pair ['science': ['science', 'space', 'physics', 'biology']] (155:12-155:65)
                  string ['science'] (155:12-155:21)
                    string_start ['] (155:12-155:13)
                    string_content [science] (155:13-155:20)
                    string_end ['] (155:20-155:21)
                  : [:] (155:21-155:22)
                  list [['science', 'space', 'physics', 'biology']] (155:23-155:65)
                    [ [[] (155:23-155:24)
                    string ['science'] (155:24-155:33)
                      string_start ['] (155:24-155:25)
                      string_content [science] (155:25-155:32)
                      string_end ['] (155:32-155:33)
                    , [,] (155:33-155:34)
                    string ['space'] (155:35-155:42)
                      string_start ['] (155:35-155:36)
                      string_content [space] (155:36-155:41)
                      string_end ['] (155:41-155:42)
                    , [,] (155:42-155:43)
                    string ['physics'] (155:44-155:53)
                      string_start ['] (155:44-155:45)
                      string_content [physics] (155:45-155:52)
                      string_end ['] (155:52-155:53)
                    , [,] (155:53-155:54)
                    string ['biology'] (155:55-155:64)
                      string_start ['] (155:55-155:56)
                      string_content [biology] (155:56-155:63)
                      string_end ['] (155:63-155:64)
                    ] []] (155:64-155:65)
                , [,] (155:65-155:66)
                pair ['tennis': ['tennis', 'ATP_Tour', 'WTA', 'TennisPro']] (156:12-156:64)
                  string ['tennis'] (156:12-156:20)
                    string_start ['] (156:12-156:13)
                    string_content [tennis] (156:13-156:19)
                    string_end ['] (156:19-156:20)
                  : [:] (156:20-156:21)
                  list [['tennis', 'ATP_Tour', 'WTA', 'TennisPro']] (156:22-156:64)
                    [ [[] (156:22-156:23)
                    string ['tennis'] (156:23-156:31)
                      string_start ['] (156:23-156:24)
                      string_content [tennis] (156:24-156:30)
                      string_end ['] (156:30-156:31)
                    , [,] (156:31-156:32)
                    string ['ATP_Tour'] (156:33-156:43)
                      string_start ['] (156:33-156:34)
                      string_content [ATP_Tour] (156:34-156:42)
                      string_end ['] (156:42-156:43)
                    , [,] (156:43-156:44)
                    string ['WTA'] (156:45-156:50)
                      string_start ['] (156:45-156:46)
                      string_content [WTA] (156:46-156:49)
                      string_end ['] (156:49-156:50)
                    , [,] (156:50-156:51)
                    string ['TennisPro'] (156:52-156:63)
                      string_start ['] (156:52-156:53)
                      string_content [TennisPro] (156:53-156:62)
                      string_end ['] (156:62-156:63)
                    ] []] (156:63-156:64)
                , [,] (156:64-156:65)
                pair ['ram': ['hardware', 'computer', 'buildapc', 'techsupport']] (157:12-157:70)
                  string ['ram'] (157:12-157:17)
                    string_start ['] (157:12-157:13)
                    string_content [ram] (157:13-157:16)
                    string_end ['] (157:16-157:17)
                  : [:] (157:17-157:18)
                  list [['hardware', 'computer', 'buildapc', 'techsupport']] (157:19-157:70)
                    [ [[] (157:19-157:20)
                    string ['hardware'] (157:20-157:30)
                      string_start ['] (157:20-157:21)
                      string_content [hardware] (157:21-157:29)
                      string_end ['] (157:29-157:30)
                    , [,] (157:30-157:31)
                    string ['computer'] (157:32-157:42)
                      string_start ['] (157:32-157:33)
                      string_content [computer] (157:33-157:41)
                      string_end ['] (157:41-157:42)
                    , [,] (157:42-157:43)
                    string ['buildapc'] (157:44-157:54)
                      string_start ['] (157:44-157:45)
                      string_content [buildapc] (157:45-157:53)
                      string_end ['] (157:53-157:54)
                    , [,] (157:54-157:55)
                    string ['techsupport'] (157:56-157:69)
                      string_start ['] (157:56-157:57)
                      string_content [techsupport] (157:57-157:68)
                      string_end ['] (157:68-157:69)
                    ] []] (157:69-157:70)
                } [}] (158:8-158:9)
          expression_statement [topic_lower = topic.lower()] (160:8-160:35)
            assignment [topic_lower = topic.lower()] (160:8-160:35)
              identifier [topic_lower] (160:8-160:19)
              = [=] (160:20-160:21)
              call [topic.lower()] (160:22-160:35)
                attribute [topic.lower] (160:22-160:33)
                  identifier [topic] (160:22-160:27)
                  . [.] (160:27-160:28)
                  identifier [lower] (160:28-160:33)
                argument_list [()] (160:33-160:35)
                  ( [(] (160:33-160:34)
                  ) [)] (160:34-160:35)
          return_statement [return topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])] (161:8-161:81)
            return [return] (161:8-161:14)
            call [topic_subreddit_map.get(topic_lower, [topic, f'{topic}subreddit'])] (161:15-161:81)
              attribute [topic_subreddit_map.get] (161:15-161:38)
                identifier [topic_subreddit_map] (161:15-161:34)
                . [.] (161:34-161:35)
                identifier [get] (161:35-161:38)
              argument_list [(topic_lower, [topic, f'{topic}subreddit'])] (161:38-161:81)
                ( [(] (161:38-161:39)
                identifier [topic_lower] (161:39-161:50)
                , [,] (161:50-161:51)
                list [[topic, f'{topic}subreddit']] (161:52-161:80)
                  [ [[] (161:52-161:53)
                  identifier [topic] (161:53-161:58)
                  , [,] (161:58-161:59)
                  string [f'{topic}subreddit'] (161:60-161:79)
                    string_start [f'] (161:60-161:62)
                    interpolation [{topic}] (161:62-161:69)
                      { [{] (161:62-161:63)
                      identifier [topic] (161:63-161:68)
                      } [}] (161:68-161:69)
                    string_content [subreddit] (161:69-161:78)
                    string_end ['] (161:78-161:79)
                  ] []] (161:79-161:80)
                ) [)] (161:80-161:81)
      function_definition [def _parse_comments_recursively(self, comments, depth=0, max_depth=3):
        """Parse comments recursively with depth limit."""
        if depth >= max_depth:
            return []

        parsed_comments = []
        for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")

        return parsed_comments] (163:4-190:30)
        def [def] (163:4-163:7)
        identifier [_parse_comments_recursively] (163:8-163:35)
        parameters [(self, comments, depth=0, max_depth=3)] (163:35-163:73)
          ( [(] (163:35-163:36)
          identifier [self] (163:36-163:40)
          , [,] (163:40-163:41)
          identifier [comments] (163:42-163:50)
          , [,] (163:50-163:51)
          default_parameter [depth=0] (163:52-163:59)
            identifier [depth] (163:52-163:57)
            = [=] (163:57-163:58)
            integer [0] (163:58-163:59)
          , [,] (163:59-163:60)
          default_parameter [max_depth=3] (163:61-163:72)
            identifier [max_depth] (163:61-163:70)
            = [=] (163:70-163:71)
            integer [3] (163:71-163:72)
          ) [)] (163:72-163:73)
        : [:] (163:73-163:74)
        block ["""Parse comments recursively with depth limit."""
        if depth >= max_depth:
            return []

        parsed_comments = []
        for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")

        return parsed_comments] (164:8-190:30)
          expression_statement ["""Parse comments recursively with depth limit."""] (164:8-164:58)
            string ["""Parse comments recursively with depth limit."""] (164:8-164:58)
              string_start ["""] (164:8-164:11)
              string_content [Parse comments recursively with depth limit.] (164:11-164:55)
              string_end ["""] (164:55-164:58)
          if_statement [if depth >= max_depth:
            return []] (165:8-166:21)
            if [if] (165:8-165:10)
            comparison_operator [depth >= max_depth] (165:11-165:29)
              identifier [depth] (165:11-165:16)
              >= [>=] (165:17-165:19)
              identifier [max_depth] (165:20-165:29)
            : [:] (165:29-165:30)
            block [return []] (166:12-166:21)
              return_statement [return []] (166:12-166:21)
                return [return] (166:12-166:18)
                list [[]] (166:19-166:21)
                  [ [[] (166:19-166:20)
                  ] []] (166:20-166:21)
          expression_statement [parsed_comments = []] (168:8-168:28)
            assignment [parsed_comments = []] (168:8-168:28)
              identifier [parsed_comments] (168:8-168:23)
              = [=] (168:24-168:25)
              list [[]] (168:26-168:28)
                [ [[] (168:26-168:27)
                ] []] (168:27-168:28)
          for_statement [for comment in comments:
            try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")] (169:8-188:52)
            for [for] (169:8-169:11)
            identifier [comment] (169:12-169:19)
            in [in] (169:20-169:22)
            identifier [comments] (169:23-169:31)
            : [:] (169:31-169:32)
            block [try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")] (170:12-188:52)
              try_statement [try:
                parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)

            except Exception as e:
                print(f"Error parsing comment: {e}")] (170:12-188:52)
                try [try] (170:12-170:15)
                : [:] (170:15-170:16)
                block [parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }

                if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )

                parsed_comments.append(parsed_comment)] (171:16-185:54)
                  expression_statement [parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }] (171:16-178:17)
                    assignment [parsed_comment = {
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }] (171:16-178:17)
                      identifier [parsed_comment] (171:16-171:30)
                      = [=] (171:31-171:32)
                      dictionary [{
                    'id': comment.id,
                    'author': str(comment.author) if comment.author else 'Deleted',
                    'body': comment.body,
                    'score': comment.score,
                    'created_utc': comment.created_utc,
                    'depth': depth
                }] (171:33-178:17)
                        { [{] (171:33-171:34)
                        pair ['id': comment.id] (172:20-172:36)
                          string ['id'] (172:20-172:24)
                            string_start ['] (172:20-172:21)
                            string_content [id] (172:21-172:23)
                            string_end ['] (172:23-172:24)
                          : [:] (172:24-172:25)
                          attribute [comment.id] (172:26-172:36)
                            identifier [comment] (172:26-172:33)
                            . [.] (172:33-172:34)
                            identifier [id] (172:34-172:36)
                        , [,] (172:36-172:37)
                        pair ['author': str(comment.author) if comment.author else 'Deleted'] (173:20-173:82)
                          string ['author'] (173:20-173:28)
                            string_start ['] (173:20-173:21)
                            string_content [author] (173:21-173:27)
                            string_end ['] (173:27-173:28)
                          : [:] (173:28-173:29)
                          conditional_expression [str(comment.author) if comment.author else 'Deleted'] (173:30-173:82)
                            call [str(comment.author)] (173:30-173:49)
                              identifier [str] (173:30-173:33)
                              argument_list [(comment.author)] (173:33-173:49)
                                ( [(] (173:33-173:34)
                                attribute [comment.author] (173:34-173:48)
                                  identifier [comment] (173:34-173:41)
                                  . [.] (173:41-173:42)
                                  identifier [author] (173:42-173:48)
                                ) [)] (173:48-173:49)
                            if [if] (173:50-173:52)
                            attribute [comment.author] (173:53-173:67)
                              identifier [comment] (173:53-173:60)
                              . [.] (173:60-173:61)
                              identifier [author] (173:61-173:67)
                            else [else] (173:68-173:72)
                            string ['Deleted'] (173:73-173:82)
                              string_start ['] (173:73-173:74)
                              string_content [Deleted] (173:74-173:81)
                              string_end ['] (173:81-173:82)
                        , [,] (173:82-173:83)
                        pair ['body': comment.body] (174:20-174:40)
                          string ['body'] (174:20-174:26)
                            string_start ['] (174:20-174:21)
                            string_content [body] (174:21-174:25)
                            string_end ['] (174:25-174:26)
                          : [:] (174:26-174:27)
                          attribute [comment.body] (174:28-174:40)
                            identifier [comment] (174:28-174:35)
                            . [.] (174:35-174:36)
                            identifier [body] (174:36-174:40)
                        , [,] (174:40-174:41)
                        pair ['score': comment.score] (175:20-175:42)
                          string ['score'] (175:20-175:27)
                            string_start ['] (175:20-175:21)
                            string_content [score] (175:21-175:26)
                            string_end ['] (175:26-175:27)
                          : [:] (175:27-175:28)
                          attribute [comment.score] (175:29-175:42)
                            identifier [comment] (175:29-175:36)
                            . [.] (175:36-175:37)
                            identifier [score] (175:37-175:42)
                        , [,] (175:42-175:43)
                        pair ['created_utc': comment.created_utc] (176:20-176:54)
                          string ['created_utc'] (176:20-176:33)
                            string_start ['] (176:20-176:21)
                            string_content [created_utc] (176:21-176:32)
                            string_end ['] (176:32-176:33)
                          : [:] (176:33-176:34)
                          attribute [comment.created_utc] (176:35-176:54)
                            identifier [comment] (176:35-176:42)
                            . [.] (176:42-176:43)
                            identifier [created_utc] (176:43-176:54)
                        , [,] (176:54-176:55)
                        pair ['depth': depth] (177:20-177:34)
                          string ['depth'] (177:20-177:27)
                            string_start ['] (177:20-177:21)
                            string_content [depth] (177:21-177:26)
                            string_end ['] (177:26-177:27)
                          : [:] (177:27-177:28)
                          identifier [depth] (177:29-177:34)
                        } [}] (178:16-178:17)
                  if_statement [if hasattr(comment, 'replies') and comment.replies:
                    parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )] (180:16-183:21)
                    if [if] (180:16-180:18)
                    boolean_operator [hasattr(comment, 'replies') and comment.replies] (180:19-180:66)
                      call [hasattr(comment, 'replies')] (180:19-180:46)
                        identifier [hasattr] (180:19-180:26)
                        argument_list [(comment, 'replies')] (180:26-180:46)
                          ( [(] (180:26-180:27)
                          identifier [comment] (180:27-180:34)
                          , [,] (180:34-180:35)
                          string ['replies'] (180:36-180:45)
                            string_start ['] (180:36-180:37)
                            string_content [replies] (180:37-180:44)
                            string_end ['] (180:44-180:45)
                          ) [)] (180:45-180:46)
                      and [and] (180:47-180:50)
                      attribute [comment.replies] (180:51-180:66)
                        identifier [comment] (180:51-180:58)
                        . [.] (180:58-180:59)
                        identifier [replies] (180:59-180:66)
                    : [:] (180:66-180:67)
                    block [parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )] (181:20-183:21)
                      expression_statement [parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )] (181:20-183:21)
                        assignment [parsed_comment['replies'] = self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )] (181:20-183:21)
                          subscript [parsed_comment['replies']] (181:20-181:45)
                            identifier [parsed_comment] (181:20-181:34)
                            [ [[] (181:34-181:35)
                            string ['replies'] (181:35-181:44)
                              string_start ['] (181:35-181:36)
                              string_content [replies] (181:36-181:43)
                              string_end ['] (181:43-181:44)
                            ] []] (181:44-181:45)
                          = [=] (181:46-181:47)
                          call [self._parse_comments_recursively(
                        comment.replies, depth + 1, max_depth
                    )] (181:48-183:21)
                            attribute [self._parse_comments_recursively] (181:48-181:80)
                              identifier [self] (181:48-181:52)
                              . [.] (181:52-181:53)
                              identifier [_parse_comments_recursively] (181:53-181:80)
                            argument_list [(
                        comment.replies, depth + 1, max_depth
                    )] (181:80-183:21)
                              ( [(] (181:80-181:81)
                              attribute [comment.replies] (182:24-182:39)
                                identifier [comment] (182:24-182:31)
                                . [.] (182:31-182:32)
                                identifier [replies] (182:32-182:39)
                              , [,] (182:39-182:40)
                              binary_operator [depth + 1] (182:41-182:50)
                                identifier [depth] (182:41-182:46)
                                + [+] (182:47-182:48)
                                integer [1] (182:49-182:50)
                              , [,] (182:50-182:51)
                              identifier [max_depth] (182:52-182:61)
                              ) [)] (183:20-183:21)
                  expression_statement [parsed_comments.append(parsed_comment)] (185:16-185:54)
                    call [parsed_comments.append(parsed_comment)] (185:16-185:54)
                      attribute [parsed_comments.append] (185:16-185:38)
                        identifier [parsed_comments] (185:16-185:31)
                        . [.] (185:31-185:32)
                        identifier [append] (185:32-185:38)
                      argument_list [(parsed_comment)] (185:38-185:54)
                        ( [(] (185:38-185:39)
                        identifier [parsed_comment] (185:39-185:53)
                        ) [)] (185:53-185:54)
                except_clause [except Exception as e:
                print(f"Error parsing comment: {e}")] (187:12-188:52)
                  except [except] (187:12-187:18)
                  as_pattern [Exception as e] (187:19-187:33)
                    identifier [Exception] (187:19-187:28)
                    as [as] (187:29-187:31)
                    as_pattern_target [e] (187:32-187:33)
                      identifier [e] (187:32-187:33)
                  : [:] (187:33-187:34)
                  block [print(f"Error parsing comment: {e}")] (188:16-188:52)
                    expression_statement [print(f"Error parsing comment: {e}")] (188:16-188:52)
                      call [print(f"Error parsing comment: {e}")] (188:16-188:52)
                        identifier [print] (188:16-188:21)
                        argument_list [(f"Error parsing comment: {e}")] (188:21-188:52)
                          ( [(] (188:21-188:22)
                          string [f"Error parsing comment: {e}"] (188:22-188:51)
                            string_start [f"] (188:22-188:24)
                            string_content [Error parsing comment: ] (188:24-188:47)
                            interpolation [{e}] (188:47-188:50)
                              { [{] (188:47-188:48)
                              identifier [e] (188:48-188:49)
                              } [}] (188:49-188:50)
                            string_end ["] (188:50-188:51)
                          ) [)] (188:51-188:52)
          return_statement [return parsed_comments] (190:8-190:30)
            return [return] (190:8-190:14)
            identifier [parsed_comments] (190:15-190:30)
      function_definition [def _save_local_copy(self, posts: list, topic: str) -> str:
        """Save local copy of posts."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{topic}_posts_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (192:4-204:23)
        def [def] (192:4-192:7)
        identifier [_save_local_copy] (192:8-192:24)
        parameters [(self, posts: list, topic: str)] (192:24-192:55)
          ( [(] (192:24-192:25)
          identifier [self] (192:25-192:29)
          , [,] (192:29-192:30)
          typed_parameter [posts: list] (192:31-192:42)
            identifier [posts] (192:31-192:36)
            : [:] (192:36-192:37)
            type [list] (192:38-192:42)
              identifier [list] (192:38-192:42)
          , [,] (192:42-192:43)
          typed_parameter [topic: str] (192:44-192:54)
            identifier [topic] (192:44-192:49)
            : [:] (192:49-192:50)
            type [str] (192:51-192:54)
              identifier [str] (192:51-192:54)
          ) [)] (192:54-192:55)
        -> [->] (192:56-192:58)
        type [str] (192:59-192:62)
          identifier [str] (192:59-192:62)
        : [:] (192:62-192:63)
        block ["""Save local copy of posts."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{topic}_posts_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (193:8-204:23)
          expression_statement ["""Save local copy of posts."""] (193:8-193:39)
            string ["""Save local copy of posts."""] (193:8-193:39)
              string_start ["""] (193:8-193:11)
              string_content [Save local copy of posts.] (193:11-193:36)
              string_end ["""] (193:36-193:39)
          expression_statement [timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")] (194:8-194:60)
            assignment [timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")] (194:8-194:60)
              identifier [timestamp] (194:8-194:17)
              = [=] (194:18-194:19)
              call [datetime.now().strftime("%Y%m%d_%H%M%S")] (194:20-194:60)
                attribute [datetime.now().strftime] (194:20-194:43)
                  call [datetime.now()] (194:20-194:34)
                    attribute [datetime.now] (194:20-194:32)
                      identifier [datetime] (194:20-194:28)
                      . [.] (194:28-194:29)
                      identifier [now] (194:29-194:32)
                    argument_list [()] (194:32-194:34)
                      ( [(] (194:32-194:33)
                      ) [)] (194:33-194:34)
                  . [.] (194:34-194:35)
                  identifier [strftime] (194:35-194:43)
                argument_list [("%Y%m%d_%H%M%S")] (194:43-194:60)
                  ( [(] (194:43-194:44)
                  string ["%Y%m%d_%H%M%S"] (194:44-194:59)
                    string_start ["] (194:44-194:45)
                    string_content [%Y%m%d_%H%M%S] (194:45-194:58)
                    string_end ["] (194:58-194:59)
                  ) [)] (194:59-194:60)
          expression_statement [filename = f"{topic}_posts_{timestamp}.json"] (195:8-195:52)
            assignment [filename = f"{topic}_posts_{timestamp}.json"] (195:8-195:52)
              identifier [filename] (195:8-195:16)
              = [=] (195:17-195:18)
              string [f"{topic}_posts_{timestamp}.json"] (195:19-195:52)
                string_start [f"] (195:19-195:21)
                interpolation [{topic}] (195:21-195:28)
                  { [{] (195:21-195:22)
                  identifier [topic] (195:22-195:27)
                  } [}] (195:27-195:28)
                string_content [_posts_] (195:28-195:35)
                interpolation [{timestamp}] (195:35-195:46)
                  { [{] (195:35-195:36)
                  identifier [timestamp] (195:36-195:45)
                  } [}] (195:45-195:46)
                string_content [.json] (195:46-195:51)
                string_end ["] (195:51-195:52)
          expression_statement [filepath = os.path.join(self.output_dir, filename)] (196:8-196:58)
            assignment [filepath = os.path.join(self.output_dir, filename)] (196:8-196:58)
              identifier [filepath] (196:8-196:16)
              = [=] (196:17-196:18)
              call [os.path.join(self.output_dir, filename)] (196:19-196:58)
                attribute [os.path.join] (196:19-196:31)
                  attribute [os.path] (196:19-196:26)
                    identifier [os] (196:19-196:21)
                    . [.] (196:21-196:22)
                    identifier [path] (196:22-196:26)
                  . [.] (196:26-196:27)
                  identifier [join] (196:27-196:31)
                argument_list [(self.output_dir, filename)] (196:31-196:58)
                  ( [(] (196:31-196:32)
                  attribute [self.output_dir] (196:32-196:47)
                    identifier [self] (196:32-196:36)
                    . [.] (196:36-196:37)
                    identifier [output_dir] (196:37-196:47)
                  , [,] (196:47-196:48)
                  identifier [filename] (196:49-196:57)
                  ) [)] (196:57-196:58)
          try_statement [try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath
        except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (198:8-204:23)
            try [try] (198:8-198:11)
            : [:] (198:11-198:12)
            block [with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)
            return filepath] (199:12-201:27)
              with_statement [with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=4, ensure_ascii=False, default=str)] (199:12-200:78)
                with [with] (199:12-199:16)
                with_clause [open(filepath, 'w', encoding='utf-8') as f] (199:17-199:59)
                  with_item [open(filepath, 'w', encoding='utf-8') as f] (199:17-199:59)
                    as_pattern [open(filepath, 'w', encoding='utf-8') as f] (199:17-199:59)
                      call [open(filepath, 'w', encoding='utf-8')] (199:17-199:54)
                        identifier [open] (199:17-199:21)
                        argument_list [(filepath, 'w', encoding='utf-8')] (199:21-199:54)
                          ( [(] (199:21-199:22)
                          identifier [filepath] (199:22-199:30)
                          , [,] (199:30-199:31)
                          string ['w'] (199:32-199:35)
                            string_start ['] (199:32-199:33)
                            string_content [w] (199:33-199:34)
                            string_end ['] (199:34-199:35)
                          , [,] (199:35-199:36)
                          keyword_argument [encoding='utf-8'] (199:37-199:53)
                            identifier [encoding] (199:37-199:45)
                            = [=] (199:45-199:46)
                            string ['utf-8'] (199:46-199:53)
                              string_start ['] (199:46-199:47)
                              string_content [utf-8] (199:47-199:52)
                              string_end ['] (199:52-199:53)
                          ) [)] (199:53-199:54)
                      as [as] (199:55-199:57)
                      as_pattern_target [f] (199:58-199:59)
                        identifier [f] (199:58-199:59)
                : [:] (199:59-199:60)
                block [json.dump(posts, f, indent=4, ensure_ascii=False, default=str)] (200:16-200:78)
                  expression_statement [json.dump(posts, f, indent=4, ensure_ascii=False, default=str)] (200:16-200:78)
                    call [json.dump(posts, f, indent=4, ensure_ascii=False, default=str)] (200:16-200:78)
                      attribute [json.dump] (200:16-200:25)
                        identifier [json] (200:16-200:20)
                        . [.] (200:20-200:21)
                        identifier [dump] (200:21-200:25)
                      argument_list [(posts, f, indent=4, ensure_ascii=False, default=str)] (200:25-200:78)
                        ( [(] (200:25-200:26)
                        identifier [posts] (200:26-200:31)
                        , [,] (200:31-200:32)
                        identifier [f] (200:33-200:34)
                        , [,] (200:34-200:35)
                        keyword_argument [indent=4] (200:36-200:44)
                          identifier [indent] (200:36-200:42)
                          = [=] (200:42-200:43)
                          integer [4] (200:43-200:44)
                        , [,] (200:44-200:45)
                        keyword_argument [ensure_ascii=False] (200:46-200:64)
                          identifier [ensure_ascii] (200:46-200:58)
                          = [=] (200:58-200:59)
                          false [False] (200:59-200:64)
                        , [,] (200:64-200:65)
                        keyword_argument [default=str] (200:66-200:77)
                          identifier [default] (200:66-200:73)
                          = [=] (200:73-200:74)
                          identifier [str] (200:74-200:77)
                        ) [)] (200:77-200:78)
              return_statement [return filepath] (201:12-201:27)
                return [return] (201:12-201:18)
                identifier [filepath] (201:19-201:27)
            except_clause [except Exception as e:
            print(f"Error saving local copy: {e}")
            return None] (202:8-204:23)
              except [except] (202:8-202:14)
              as_pattern [Exception as e] (202:15-202:29)
                identifier [Exception] (202:15-202:24)
                as [as] (202:25-202:27)
                as_pattern_target [e] (202:28-202:29)
                  identifier [e] (202:28-202:29)
              : [:] (202:29-202:30)
              block [print(f"Error saving local copy: {e}")
            return None] (203:12-204:23)
                expression_statement [print(f"Error saving local copy: {e}")] (203:12-203:50)
                  call [print(f"Error saving local copy: {e}")] (203:12-203:50)
                    identifier [print] (203:12-203:17)
                    argument_list [(f"Error saving local copy: {e}")] (203:17-203:50)
                      ( [(] (203:17-203:18)
                      string [f"Error saving local copy: {e}"] (203:18-203:49)
                        string_start [f"] (203:18-203:20)
                        string_content [Error saving local copy: ] (203:20-203:45)
                        interpolation [{e}] (203:45-203:48)
                          { [{] (203:45-203:46)
                          identifier [e] (203:46-203:47)
                          } [}] (203:47-203:48)
                        string_end ["] (203:48-203:49)
                      ) [)] (203:49-203:50)
                return_statement [return None] (204:12-204:23)
                  return [return] (204:12-204:18)
                  none [None] (204:19-204:23)
